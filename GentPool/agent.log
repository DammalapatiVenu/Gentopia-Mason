INFO:root:Creating Config from file: ./gentpool/pool/agent_dv_pdfreader/agent.yaml
INFO:root:Starting to load configuration from ./gentpool/pool/agent_dv_pdfreader/agent.yaml
INFO:root:Successfully loaded configuration from ./gentpool/pool/agent_dv_pdfreader/agent.yaml
INFO:root:--------------------
INFO:root:[green]Welcome to Gentopia!
INFO:root:--------------------
INFO:root:[green]User: 
INFO:root:agent_dv_pdfreader is thinking...
INFO:root:Done
INFO:root:[blue]agent_dv_pdfreader: 
INFO:root:Hello! How can I assist you today?
INFO:root:Done
INFO:root:[green]User: 
INFO:root:agent_dv_pdfreader is thinking...
INFO:root:Done
INFO:root:[blue]agent_dv_pdfreader: 
INFO:root:{"name":"document_reader", "arguments":{"qry":"https://arxiv.org/pdf/2201.05966.pdf"}}

INFO:root:Calling function: document_reader ...
INFO:root:Done
INFO:root:--------------------
INFO:root:UNIFIEDSKG: Unifying and Multi-Tasking Structured Knowledge
Grounding with Text-to-Text Language Models
Tianbao Xie∗1 Chen Henry Wu∗2 Peng Shi3 Ruiqi Zhong4 Torsten Scholak5
Michihiro Yasunaga6 Chien-Sheng Wu7 Ming Zhong8 Pengcheng Yin9 Sida I. Wang10
Victor Zhong17 Bailin Wang11 Chengzu Li12 Connor Boyle17 Ansong Ni13 Ziyu Yao14
Dragomir Radev13 Caiming Xiong7 Lingpeng Kong1,12 Rui Zhang15
Noah A. Smith16,17 Luke Zettlemoyer10,17 Tao Yu1,17
1The University of Hong Kong
2Carnegie Mellon University
3University of Waterloo
4UC Berkeley
5ServiceNow Research
6Stanford University
7Salesforce Research
8UIUC
9Google Research
10Facebook AI Research
11University of Edinburgh
12Shanghai AI Lab
13Yale University
14George Mason University
15Penn State University
16Allen Institute for Artiﬁcial Intelligence
17University of Washington
Abstract
Structured knowledge grounding (SKG) lever-
ages structured knowledge to complete user
requests,
such as semantic parsing over
databases and question answering over knowl-
edge bases.
Since the inputs and outputs
of SKG tasks are heterogeneous, they have
been studied separately by different communi-
ties, which limits systematic and compatible
research on SKG. In this paper, we overcome
this limitation by proposing the UNIFIEDSKG
framework, which uniﬁes 21 SKG tasks into
a text-to-text format, aiming to promote sys-
tematic SKG research, instead of being exclu-
sive to a single task, domain, or dataset. We
use UNIFIEDSKG to benchmark T5 with dif-
ferent sizes and show that T5, with simple
modiﬁcations when necessary, achieves state-
of-the-art performance on almost all of the 21
tasks. We further demonstrate that multi-task
preﬁx-tuning improves the performance on
most tasks, largely improving the overall per-
formance.
UNIFIEDSKG also facilitates the
investigation of zero-shot and few-shot learn-
ing, and we show that T0, GPT-3, and Codex
struggle in zero-shot and few-shot learning
for SKG. We also use UNIFIEDSKG to con-
duct a series of controlled experiments on
structured knowledge encoding variants across
SKG tasks. UNIFIEDSKG is easily extensible
to more tasks, and it is open-sourced at https:
//github.com/hkunlp/unifiedskg.1
1
Introduction
Structured knowledge (e.g., web tables, knowledge
graphs, and databases) stores large amounts of data
in organized structures, forming a basis for a wide
range of applications, e.g., medical diagnosis, per-
sonal assistants, and customer relations manage-
∗Equal contributions. Author contributions in App. A.
1Latest collections at https://unifiedskg.com.
ment. Accessing and searching data in structured
knowledge typically requires mastering query lan-
guages through professional training. To promote
the efﬁciency of data access, structured knowledge
grounding (SKG) systems ground user requests
in structured knowledge and produce various out-
puts, including computer programs (e.g., SQL and
SPARQL), table cell values, and natural language
responses (Figure 1). For example, semantic pars-
ing (Zelle and Mooney, 1996; Zettlemoyer and
Collins, 2005) converts natural language questions
into formal programs; knowledge-base question an-
swering (Berant et al., 2013) derives answers from
tables or knowledge graphs.
SKG has attracted signiﬁcant interest and has
been studied through different tasks deﬁned by dif-
ferent communities. Recent developments in tasks,
models, and datasets for SKG have led to task-
speciﬁc modeling advances, making each task’s
progress seemingly unique and incompatible. A
main reason is that SKG tasks are heterogeneous.
Different types of structured knowledge, such as
databases or knowledge graphs, lead to highly spe-
cialized encoders (Lin et al., 2019; Herzig et al.,
2020; Wang et al., 2020; Yasunaga et al., 2021).
Some SKG tasks, e.g., semantic parsing, use cus-
tomized decoders to generate programs (Yin and
Neubig, 2018; Ren et al., 2021). Therefore, instead
of solving common challenges in SKG research,
improvements in SKG have been prone to be exclu-
sive to a single task, domain, or dataset.
In this paper, we propose the UNIFIEDSKG
framework to advocate for a unifying view of 21
SKG tasks across six task families and multiple
data domains (Table 1). UNIFIEDSKG standardizes
datasets, models, code, experiments, and evalua-
tion metrics into a single framework. By casting
user requests, structured knowledge, and outputs
arXiv:2201.05966v3  [cs.CL]  18 Oct 2022


147
Structured Knowledge Grounding
knowledge graphs
web tables/pages
databases/apps
Structured Knowledge
Greece held its last Summer 
Olympics in which year?
Question Answering
Describe the table result.
Data-to-Text Generation
Canada obtained 3 more 
gold medals than Mexico.
Fact Veriﬁcation
      I am looking for a cheap           
      restaurant in the city center.
      Book a table for 8 at 18:30 on   
      Thursday.
Dialogs
       Which players did win the  
       Australian Open?
Semantic Parsing
SELECT T1.name 
FROM players AS T1 JOIN matches AS T2 
ON T1.id = T2.winner_id 
WHERE T2.Tourney = “Australian Open” 
SQL/SPARQL/s-Expression
2014
Answer set
In 1970, Hawaii's population mainly 
consists of 38.8% White and 
57.7% Asian, Native Hawaiian...
NL description
False
Boolean
Restaurant(price=cheap,area=center)
Restaurant(price=cheap,area=center,    
           name=Dojo Noodle Bar,   
           people=8, time=18:30, 
           day=Thursday)
Multi-turn SQL-like programs
UniﬁedSKG
Figure 1: Structured knowledge grounding (SKG) leverages structured knowledge to complete user requests. By
casting inputs and outputs into the text-to-text format, UNIFIEDSKG standardizes datasets, models, code, experi-
ments, and metrics for 21 SKG tasks.
into the text-to-text format (Raffel et al., 2020), it
promotes model advances where new tasks can be
framed with our standardized abstraction, and new
models can be easily applied to diverse SKG tasks.
While previous works also cast SKG tasks into the
text-to-text format (Hosseini-Asl et al., 2020; Shaw
et al., 2021; Liu et al., 2021), their independent
choices of pretrained language models (PLMs),
input-output formats, and frameworks make our
uniﬁcation non-trivial. UNIFIEDSKG is easily ex-
tensible to more SKG tasks, and it is open-sourced
to promote community-wide progress.
Using UNIFIEDSKG as a benchmark, we show
that ﬁnetuning T5 (with constrained decoding or
reranking when necessary) on individual tasks
achieves state-of-the-art (sota) results on almost
all of the 21 tasks, establishing a powerful and
reproducible starting point for SKG research. T5
performance also increases with size on most tasks.
UNIFIEDSKG facilitates multi-task learning on
SKG, enabling knowledge sharing and cross-task
generalization. Although simple multi-task learn-
ing has mixed results, we show that multi-task
learning with preﬁx-tuning (Li and Liang, 2021)
beneﬁts most tasks and largely improves the overall
performance, on both T5-base and T5-large.
UNIFIEDSKG is a challenging testbed for few-
shot (Brown et al., 2020; Ye et al., 2021a) and
zero-shot learning (Zhong et al., 2021; Wei et al.,
2021; Sanh et al., 2021) with PLMs. Our experi-
ments show that models like T0 (Sanh et al., 2021)
struggle in zero-shot learning on SKG tasks, and
GPT-3 (Brown et al., 2020) and Codex (Chen et al.,
2021a) struggle in few-shot learning on SKG tasks.
UNIFIEDSKG enables a series of controlled ex-
periments on structured knowledge encoding. We
ﬁnd that T5 is sensitive to encoding variations, and
the sensitivity varies across tasks. UNIFIEDSKG
aims to facilitate more general and robust struc-
tured knowledge encoding methods. Finally, we
conduct a comprehensive error analysis across
SKG tasks. Although the errors made by PLMs
decrease with the model size, T5-3B may still gen-
erate invalid outputs.
In summary, we 1) unify and benchmark 21 SKG
tasks under the UNIFIEDSKG framework to evalu-
ate diverse grounding goals and structured knowl-
edge sources, 2) demonstrate (near) sota perfor-
mance of T5 on all the uniﬁed SKG tasks, using
a single, general-purpose approach, 3) show the
beneﬁt of knowledge sharing across SKG tasks
via multi-task preﬁx-tuning, and 4) analyze recent
modeling contributions (zero-shot, few-shot, and
structured knowledge encoding) on these tasks. We
hope UNIFIEDSKG enables the design of new mod-
els and learning algorithms that generalize to di-
verse SKG tasks and to identify their challenges.
2
Related Work
SKG with PLMs
PLMs have been applied to sev-
eral SKG tasks. To encode structured knowledge,
prior work linearized the structured knowledge and
concatenated it with the text (Hwang et al., 2019;
Liu et al., 2020; Hosseini-Asl et al., 2020; Liu et al.,
2021), which has been augmented by positional
encoding (e.g., row/column embedding) (Herzig
et al., 2020; Yin et al., 2020a) and template-based
linearization (Chen et al., 2020a,b; Oguz et al.,
2021), and planning (Su et al., 2021). Recently,
cell-column alignment is modeled by manipulating


Task Family
Task
Knowledge Input
User Input
Output
Semantic Parsing
Spider (Yu et al., 2018)
Database
Question
SQL
GrailQA (Gu et al., 2021)
Knowledge Graph
Question
s-Expression
WebQSP (Yih et al., 2016)
Knowledge Graph
Question
s-Expression
MTOP (Li et al., 2021)
API Calls
Question
TOP Representation
Question Answering
WikiSQL (Zhong et al., 2017)
Table
Question
Answer
WikiTQ (Pasupat and Liang, 2015)
Table
Question
Answer
CompWebQ (Talmor and Berant, 2018)
Knowledge Graph
Question
Answer
HybridQA (Chen et al., 2020c)
Table + Text Passage
Question
Answer
MultiModalQA (Talmor et al., 2021)
Table + Text + Image
Question
Answer
FeTaQA (Nan et al., 2021a)
Table
Question
Free-Form Answer
Data-to-Text
DART (Nan et al., 2021b)
Triple
None
Text
ToTTo (Parikh et al., 2020)
Highlighted Table
None
Text
MultiWoZ (Budzianowski et al., 2018)
Ontology
Dialog
Dialog State
KVRET (Eric et al., 2017)
Table
Dialog
Response
Conversational
SParC (Yu et al., 2019b)
Database
Multi turn
SQL
CoSQL (Yu et al., 2019a)
Database
Dialog
SQL
SQA (Iyyer et al., 2017)
Table
Multi turn
Answer
Fact Veriﬁcation
TabFact (Chen et al., 2020b)
Table
Statement
Boolean
FEVEROUS (Aly et al., 2021)
Table + Text
Statement
Boolean
Formal-Language-to-Text
SQL2Text (Shu et al., 2021)
Optional Database
SQL
Text
Logic2Text (Chen et al., 2020d)
Table Schema
Python-like program
Text
Table 1: We unify 21 SKG tasks with different knowledge input, user input, and output, covering six task families.
the attention matrix of transformers (Zhang et al.,
2020; Eisenschlos et al., 2021). Hierarchical encod-
ing is another way to represent the structure, e.g.,
Wang et al. (2021b) used tree-based transformers
to represent the structure of the tables; Iida et al.
(2021) used transformers to encode row and col-
umn representations; Chen et al. (2021b) used hier-
archical transformers to encode KG triples. SKG’s
outputs include, but are not limited to, structured
meaning representations (e.g., logic forms, SQL),
dialogue states, natural language, answer sets, and
Boolean values. Among them, structured mean-
ing representation is challenging for PLMs because
they are originally trained on natural language. To
bridge this gap, Shin et al. (2021) adopted the in-
sights from Berant and Liang (2014) and Marzoev
et al. (2020) and proposed to convert formal lan-
guage into an English-like representation, decode
with GPT-3, and map back to formal language au-
tomatically. We do not focus on these techniques
in this work; instead, we unify all tasks and system-
atically compare them.
Task format uniﬁcation
Recent years witnessed
the trend of unifying related but different tasks into
a shared format. McCann et al. (2018) uniﬁed vari-
ous tasks as question answering. Yin et al. (2020b)
and Wang et al. (2021a) uniﬁed few-shot learning
as textual entailment. PLUR (Chen et al., 2021c)
uniﬁed program learning, understanding, and repair
tasks into a graph-to-sequence format. In this paper,
we focus on the text-to-text format (Raffel et al.,
2020) due to its ﬂexibility. Different from unifying
tasks that only take text as input, a core challenge
in unifying SKG tasks into the text-to-text format
is to linearize structured knowledge. Notably, Uni-
ﬁedQA (Khashabi et al., 2020) uniﬁed QA tasks,
while UNIFIEDSKG covers a broader scope of six
task families for systematic exploration.
Cross-task generalization with PLMs
Multi-
task learning and transfer learning go beyond task
boundaries, view different tasks as related, and
have been shown to outperform single-task learning
(Aghajanyan et al., 2021a; Vu et al., 2021). Large
PLMs show potential for zero-shot and few-shot
learning, e.g., GPT-2 (Radford et al., 2019) and
GPT-3 (Brown et al., 2020), which can be improved
by multi-task learning (Zhong et al., 2021), e.g.,
FLAN (Wei et al., 2021), T0 (Sanh et al., 2021),
and CrossFit (Ye et al., 2021a). ExT5 (Aribandi
et al., 2021) shows that scaling up multi-task learn-
ing helps improve pretraining efﬁciency and down-
stream performances. UNIFIEDSKG facilitates the
investigation of multi-task, zero-shot, and few-shot
learning on SKG tasks.
3
The UNIFIEDSKG Framework
3.1
Task Uniﬁcation
The guiding principle of UNIFIEDSKG’s task se-
lection is diversity. We unify 21 SKG tasks across
six task families and multiple domains (Table 1).
Our task families include:
• Semantic parsing converts questions to logical
forms (Zelle and Mooney, 1996; Zettlemoyer and
Collins, 2005).


True
Entailed
False
Refuted
Boolean
Natural language
How many singers are there?
Formal language
select count (*) from singer
Set of answers
Dialogue states 
{ New York, Paris } 
New York, Paris
hotel type none, hotel name Wartworth
hotel type: none
hotel name: Wartworth
Mars Hill College : joined : 1973 | 
Mars Hill College : location : Mars Hill
Tables
col : player | no. 
row 1 : antonio | 21 
row 2 : voshon | 2
Triples
(Mars Hill College, joined, 1973)
(Mars Hill College, location, Mars Hill)
player
no.
antonio
21
voshon
2
hotel-pricerange: cheap, expensive, dontcare; 
Hotel-parking: free, no, yes, dontcare
hotel-pricerange from {cheap, expensive dontcare}
hotel-parking from {free, no, yes, dontcare}
Cut Bank film.cinematography Ben Richardson | 
Cut Bank film.language English
Knowledge graph 
Ontology
Cut Bank 
Ben Richardson
English
film.cinematography
film.language
; structured knowledge:                ; context:
User request
Context
Unified output sequence !𝑦
Unified input sequence !𝑥
Text-to-text PLM
Linearized structured knowledge
Figure 2: We unify SKG tasks with heterogeneous inputs and outputs into the text-to-text format.
• Question answering derives answers to natural
language questions based on structured data (Be-
rant et al., 2013).
• Data-to-text generation describes structured
data in natural language (Novikova et al., 2017).
• Fact veriﬁcation checks if a statement is true
based on the structured data (Chen et al., 2020b).
• Conversational tasks require understanding of
not only the user’s last request but also the full
interaction history between users and machines
(Budzianowski et al., 2018; Eric et al., 2019; Yu
et al., 2019a).
• Formal language to text translation describes
formal language in natural language (Chen et al.,
2020d).
All these tasks take as input x a user request, a
structured knowledge input, and an optional (di-
alogue) context to predict an output y. Figure 2
illustrates how we convert the input x to an in-
put sequence ˜x and the output y to an output se-
quence ˜y by means of “linearization” (Liu et al.,
2021), enabling the uniﬁcation of diverse forms of
structured knowledge. We provide more details,
examples, and input length analysis in the Appen-
dices F and G. Our code implementation uses Hug-
ging Face’s Transformers (Wolf et al., 2020) and
Datasets (Lhoest et al., 2021) toolkits.
3.2
Modeling
The simplest usage of UNIFIEDSKG is to train on
individual tasks. In this case, we minimize the
negative log-likelihood loss averaged over tokens
in each batch. For decoding, we use beam search
by default. UNIFIEDSKG also facilitates explo-
ration of multi-task learning, few-shot, and zero-
shot learning with PLMs, and details are presented
in the corresponding parts in Section 4.
4
Experiments and Analysis
4.1
Results on Individual Tasks
We apply T5 models (Raffel et al., 2020) on each
individual task in UNIFIEDSKG. For model train-
ing, we set the maximum number of epochs as
50–200, depending on the dataset size. We use
early stopping and model selection on the devel-
opment set. More details are shown in Appendix
D.1. For each task, we report one commonly used
metric in Table 2. See Appendix B for all metrics.
Comparison with previous sota
Table 2 shows
that vanilla T5-3B outperforms most previous
sota models not trained on extra unsupervised in-
domain data. Some semantic parsing sota models,
denoted as + in Table 2, are also T5 with con-
strained decoding (Scholak et al., 2021) or rerank-
ing (Ye et al., 2021b). This shows that a generalist
architecture like T5, when scaled up to a certain
size, can be as good as task-speciﬁc architectures
for SKG, suggesting the potential of larger PLMs.
Model scalability
In general, T5 performance
increases with the model size, but this trend varies
across task families. Semantic parsing, QA, and
fact veriﬁcation tasks get large beneﬁts from in-
creased sizes, while text generation does not. See
Section 4.5 for a human evaluation for text genera-
tion tasks. Also, the gap between T5-base (220M)
and T5-large (770M) is larger than the gap between
T5-large (770M) and T5-3B (3B).
Effect of pretraining on structured knowledge
Some smaller models pretrained on structured
knowledge (Liu et al., 2021) show competitive
performance as T5-3B, suggesting that pretrain-
ing with structured data is beneﬁcial for SKG. This
result calls for structured knowledge pretraining
that generalizes to different SKG tasks across do-
mains, which can be systematically explored using
UNIFIEDSKG.


Metric
T5-base
T5-large
T5-3B
Previous sota (w/o extra)
Previous sota (w/ extra)
Spider (dev.)
Match
58.12
66.63
71.76
75.5+ (Scholak et al., 2021)
74.7 (Rubin and Berant, 2021)
GrailQA
Match
62.39
67.30
70.11
83.8+ (Ye et al., 2021b)
—
WebQSP
F1
78.83
79.45
80.70
83.6+ (Ye et al., 2021b)
—
MTOP
Match
85.49
86.17
86.78
86.36 (Pasupat et al., 2021)
—
WikiTQ
Acc
35.76
43.22
49.29
44.5 (Wang et al., 2019)
57.5 (Liu et al., 2021)
WikiSQL
Acc
82.63
84.80
85.96
85.8 (Liu et al., 2021)
89.5 (Liu et al., 2021)
CompWebQ
Acc
68.43
71.38
73.26
70.4‡ (Das et al., 2021)
—
HybridQA (dev.)
Acc
54.07
56.95
59.41
60.8‡ (Eisenschlos et al., 2021)
63.4‡ (Eisenschlos et al., 2021)
MultiModalQA (dev.)
F1
75.51
81.84
85.28
82.7 (Yoran et al., 2021)
83.8 (Yoran et al., 2021)
FeTaQA
BLEU
29.91
32.45
33.44
30.54 (Nan et al., 2021a)
—
DART
BLEU
46.22
46.89
46.66
46.89 (Nan et al., 2021b)
47.2 (Aghajanyan et al., 2021b)
ToTTo (dev.)
BLEU
48.29
48.95
48.95
48.95 (Kale and Rastogi, 2020)
—
MultiWoZ2.1
Joint Acc
54.64
54.45
55.42
60.61∗(Dai et al., 2021)
60.48 (Yu et al., 2021)
KVRET
Micro F1
66.45
65.85
67.88
63.6 (Gou et al., 2021)
—
SParC (dev.)
Match
50.54
56.69
61.51
54.1 (Hui et al., 2021)
62.2 (Yu et al., 2021)
CoSQL (dev.)
Match
42.30
48.26
54.08
56.9+ (Scholak et al., 2021)
52.1 (Yu et al., 2021)
SQA
Overall Acc
52.91
61.28
62.37
58.6 (Liu et al., 2021)
74.5 (Liu et al., 2021)
TabFact
Acc
76.13
80.85
83.68
74.4 (Yang et al., 2020)
84.2 (Liu et al., 2021)
FEVEROUS (dev.)
Acc
75.05
79.81
82.40
82.38 (Aly et al., 2021)
—
SQL2Text
BLEC
93.52
93.68
94.78
93.7 (Shu et al., 2021)
—
Logic2Text
BLEC
90.66
90.57
91.39
88.6 (Shu et al., 2021)
—
Table 2: Test or development (dev.) set performance of models trained on individual tasks. Vanilla T5 or T5 with
simple modiﬁcations (e.g., +constrained decoding or reranking) achieve sota on nearly all tasks. The best result
without extra pretraining is shown in bold. More detailed results and result variances can be found in Tables 11
and 12 in Appendix. Human evaluation for generation tasks is in Section 4.5. w/ (w/o) extra means with (without)
extra pretraining on unsupervised structured data (e.g., web tables).2
Spider WikiTQ DART MWoZ TabFact SQL2Text
T5-3B
71.76
50.65
50.38
58.46
83.97
92.71
T0-3B
68.09
50.62
50.16
60.20
85.51
92.93
Table 3: Comparison between T5-3B and T0-3B. T0-
3B is initialized from LM-adapted T5 and further pre-
trained on a large number of non-SKG tasks. We ﬁne-
tune both models on individual tasks. T0-3B under-
performs T5-3B on semantic parsing (Spider) and out-
performs T5-3B on dialogue state tracking (MWoZ)
and fact veriﬁcation (TabFact). We report results on
the dev. set.
Effect of pretraining on non-SKG tasks
T0-3B
(Sanh et al., 2021) is initialized from T5-3B and
pretrained on multiple tasks that (in most cases) do
not use structured knowledge as input (non-SKG
tasks). Exploring the performance of T0-3B on
SKG tasks helps us understand the relationship
between SKG tasks and non-SKG tasks. Table 3
shows that T0-3B under-performs T5-3B on se-
mantic parsing and outperforms T5-3B on dialogue
state tracking and fact veriﬁcation. We note that
T0-3B is pretrained on dialogue QA, dialogue sum-
marization, and NLI tasks; therefore, pretraining
on non-SKG tasks might not be useful for SKG
unless we add similar SKG tasks to pretraining.
2For GrailQA and WebQSP, we run T5 and rerun the pre-
vious sota model (Ye et al., 2021b) using the gold entities. For
4.2
Multi-Task Learning
UNIFIEDSKG facilitates the exploration of multi-
task learning. In this part, we systematically study
multi-task learning on all 21 uniﬁed tasks. We ﬁnd
that SKG beneﬁts from multi-task preﬁx-tuning on
both T5-base and T5-large, showing that the bene-
ﬁts from multi-task learning is scalable in terms of
the model size. The baselines we use include:
Single-task ﬁnetuning (ST-F), which is ﬁnetun-
ing on individual tasks, same as Section 4.1.
Single-task preﬁx-tuning (ST-P; Li and Liang,
2021), which learns lightweight task-speciﬁc pa-
MultiModalQA and FEVEROUS, we report performance of
T5 and the previous sota models on the dev. samples with at
least one table (samples with image input are further excluded
for MultiModalQA); The gold table and text candidates are
used for both T5 and previous sota (for MultiModelQA, num-
bers are from (Yoran et al., 2021), and for FEVEROUS, we
rerun the available model (Aly et al., 2021) on gold candi-
dates to obtain the number). We use sacreBLEU to report all
BLEU results. ‡We use gold entity linking, but the previous
sota does not, which makes the results not directly compara-
ble; therefore, we do not bold any numbers for CompWebQ
and HybridQA. ∗T5-base with the independent output scheme
(Lee et al., 2021) achieves 56.66 on MWoZ2.1, higher than our
sequence output scheme. For WebQSP, as the original dataset
does not have a dev. set, we split the original train set into
in-house train/dev. sets (90%/10%), following prior practice
(e.g. Ren et al. (2021)). Similarly, for CompWebQ, as the test
set is not publicly available, we split the original dev. set into
in-house dev./test sets (20%/80%). For GrailQA, we split the
original dev. set into in-house dev./test sets (5%/95%).


rameters while keeping the PLM ﬁxed. We set the
preﬁx length as 10. Clive et al. (2021) also used
preﬁx-tuning on T5 for data-to-text generation.
Multi-task ﬁnetuning (MT-F), which combines
the training data of all tasks with temperature mix-
ing (Raffel et al., 2020; after hyperparameter tuning
with a few steps, we set the temperature as 2). We
select model weights based on the average metric
on all tasks’ development set.
Table 4 shows that ST-P is comparable to ST-F
on nearly all tasks. However, we ﬁnd that it takes
about 5–10 times as many training steps (See Ap-
pendix E), which is similarly observed for prompt-
tuning (Lester et al., 2021). We also observe that
MT-F leads to mixed results. For many tasks, MT-F
is even worse than ST-F.
Multi-task preﬁx-tuning (MT-P)
Our explana-
tion for the mixed results of MT-F is that the inputs
of SKG tasks contain different structured knowl-
edge from diverse domains, making it difﬁcult to
learn shared parameters effectively. To address this
challenge, we ﬁrst pretrain a preﬁx on all tasks,
freezing T5 and using the same temperature mix-
ing as MT-F. In the second step, we initialize each
task’s preﬁx with this pretrained preﬁx and opti-
mize the preﬁx while freezing T5. This initializa-
tion step is similar to the prompt transfer explored
in Vu et al. (2021). Following ST-P, we set the
preﬁx length as 10.
Table 4 shows that multi-task preﬁx-tuning out-
performs single-task ﬁnetuning and single-task
preﬁx-tuning on most tasks, and it largely outper-
forms the naive multi-task learning baseline. It
demonstrates that SKG tasks can be studied to-
gether to share data and knowledge.
Exploring task knowledge transfer
UNI-
FIEDSKG facilitates studying knowledge transfer
between SKG tasks. Given two tasks, task A and
task B, we ﬁrst train the model on task A and then
continue training on task B. Table 5 shows that
tasks beneﬁt from other tasks with the same data
source (e.g., tasks that all use Wikipedia tables as
structured knowledge). We do not observe posi-
tive transfer between parallel tasks (e.g., semantic
parsing tasks with different structured knowledge
and different output) and subtask (e.g., question
answering can be viewed as the execution semantic
parses) when data sources are different. Compared
to the positive results in Table 4, results in this part
indicate that manually selecting source and target
tasks may not be efﬁcient for multi-task learning.
T5-base
T5-large
ST-F
ST-P
MT-F
MT-P
ST-F
MT-P
Spider
58.12
58.61
58.90
59.86
66.63
67.60
GrailQA
60.00
61.33
56.00
62.67
67.00
65.33
WebQSP
72.50
73.81
67.25
74.77
73.96
74.92
MTOP
83.89
82.93
78.79
82.77
84.70
84.34
WikiTQ
36.94
36.42
41.15
39.74
43.30
50.90
WikiSQL
84.50
83.09
81.85
84.44
86.27
87.45
CompWQ
66.71
67.85
68.28
69.70
68.85
71.27
HybridQA
54.07
54.93
53.52
54.88
56.95
57.33
MMQA
75.51
75.50
76.63
76.40
81.84
84.59
FeTaQA
29.00
28.03
31.85
29.33
30.94
32.48
DART
50.62
50.33
49.74
50.68
51.72
50.82
ToTTo
48.29
45.70
45.29
45.21
48.95
47.90
MWoZ2.1
57.52
56.67
53.19
57.06
58.23
59.24
KVRET
20.04
19.68
18.53
21.32
18.84
20.76
SParC
50.54
51.04
51.70
51.29
56.69
59.02
CoSQL
42.30
44.39
43.59
45.68
48.26
51.64
SQA
49.49
44.81
51.48
48.43
59.12
58.15
TabFact
76.34
75.74
71.19
77.86
81.40
83.62
FEVER.
75.05
75.33
76.85
78.02
79.81
82.05
SQL2Text
93.69
94.50
93.57
93.79
93.35
93.93
Logic2Text
92.15
95.25
92.24
94.70
92.88
93.61
Total para.
21T
T + 21P
T
T + 21P
21T
T + 21P
Avg. score
60.82
60.76
60.08
61.84
64.27
65.57
Table 4: Multi-task learning results. ST and MT stand
for single-task and multi-task. F and P stand for ﬁne-
tuning and preﬁx-tuning. For total parameters, T and P
are the numbers of T5 and preﬁx parameters (P ≪T).
Multi-task learning with preﬁx improves the perfor-
mance on most tasks, largely improving the overall per-
formance. We report results on the dev. set.
Task A
Task B
Type
B only
A to B
WikiSQL
TabFact
same source
81.43
82.76
TabFact
WikiTQ
same source
43.30
45.88
WikiSQL
FeTaQA
same source
30.94
31.19
Spider
GrailQA
parallel tasks
67.00
67.00
Spider
WikiTQ
subtask
43.30
41.68
Spider
TabFact
weakly related
81.43
80.39
Table 5: Task knowledge transfer. We use T5-large
here. B only means training the model on task B; A to B
means to train the model on task A and then to ﬁnetune
the model on task B. In both settings, we report task
B’s development set performance. We ﬁnd that tasks
beneﬁt from other tasks with the same data source.
4.3
Zero-Shot and Few-Shot Learning
The text-to-text uniﬁcation of UNIFIEDSKG en-
ables us to investigate zero/few-shot learning on
SKG with large PLMs.
Zero-shot learning setting
Zero-shot learning
enables models to solve tasks with natural language
descriptions without training samples. We follow
T0 (Sanh et al., 2021) to create similar natural lan-
guage instructions for the unseen tasks. Our in-
structions are provided in Appendix D.3.
Few-shot learning settings
Brown et al. (2020)
showed that large PLMs could be few-shot learners


T5-3B
T0 3B
GPT-3 175B
Codex 175B
ﬁnetune
zero-shot
select
random
select
random
Spider
71.76
0.00
20.00
18.333.78
40.72
43.234.16
WikiTQ
50.65
12.68
32.00
29.339.04
26.21
20.464.21
DART
50.38
23.42
40.23
34.214.50
42.13
36.541.67
MWoZ
58.46
0.00
18.00
0.020.02
23.47
0.060.03
TabFact
83.97
52.45
51.00
49.673.79
50.97
51.581.59
SQL2Text 92.71
39.64
94.00
85.002.65
90.64
88.311.61
Table 6: Zero-shot and few-shot learning for SKG. Sub-
scripts show the standard deviation with three runs. se-
lect means to select the most similar training samples as
few-shot examples, while random means to randomly
select training samples as few-shot examples. T0 per-
forms poorly on all the tasks in the zero-shot setting.
Codex outperforms GPT-3 on tasks that generate struc-
tured programs (Spider and MultiWoZ).
by encoding a few training samples as “context”
to learn without gradient updates. We use GPT-
3 (Brown et al., 2020) and Codex (Chen et al.,
2021a) to explore such few-shot learning for SKG.
To stay within our budget, for GPT-3, we report the
performance on 100 random dev. set samples. We
explore two settings for few-shot learning.
In the ﬁrst setting, we randomly sample few-shot
examples from the training set; these examples are
shared by all dev. set samples, denoted as random
in Table 6. For sequences that are too long for
Codex (4096) and GPT-3 (2048), we use as many
examples as possible and make sure that there is at
least one example (truncated if needed).
In the second setting, we follow Gao et al. (2021)
to select few-shot examples from the training set.
We call this setting few-shot with example selection,
denoted as select in Table 6. We use the pretrained
SBERT (Reimers and Gurevych, 2020) for sen-
tence embeddings of the user request input (for
tasks that only have structured input, we embed the
linearized structured input) and sample ﬁve most
similar examples measured by cosine similarity.
Further details (e.g., prompts and task instructions)
are provided in Appendix D.4.
SKG is challenging for zero/few-shot learning.
Table 6 shows that zero-shot performance is very
poor on most tasks (Spider and MultiWoZ are
even 0). It also shows a large gap between few-
shot learning and ﬁnetuning for Spider, WikiTQ,
MWoZ, and TabFact, while the gap is smaller for
generation tasks. For few-shot learning, example
selection based on similarity outperforms random
selection, but the gap is usually smaller than 10
points out of 100. It is also interesting to compare
the results between synthesis tasks (Spider), which
requires predicting programs, and induction tasks
Spider
WikiTQ
MultiWoZ2.1
TabFact
rs(c)
66.632.31
43.300.25
58.230.39
81.430.16
sr
64.12
38.78
—
80.98
rcs
—
—
58.89
—
Table 7: Ordering of inputs. Subscripts show the stan-
dard deviation with three runs. s, r, and c stand for
the structured knowledge, request input, and context.
Placing r before s is always better, and placing c be-
tween r and s is better for dialogue state tracking (Mul-
tiWoZ2.1).
Spider
WikiTQ
DART
MultiWoZ2.1
Same Order
66.632.31
43.300.25
51.720.15
58.230.39
Reversed Order
64.80
37.80
48.47
13.59
Table 8:
Order-sensitivity of structured knowledge.
Subscripts show the standard deviation with three runs.
Same Order is the default benchmark setting.
Re-
versed Order means to reverse the structured knowl-
edge ordering on the development set (but not the train-
ing set). Tasks with cross-domain tables (in WikiTQ),
databases (in Spider), and triples (in DART) are less
order-sensitive, while pre-deﬁned ontology (in Multi-
WoZ2.1) is highly order-sensitive.
(WikiTQ and TabFact), where a model directly out-
puts answers (Devlin et al., 2017). We ﬁnd that
PLMs generally struggle more when adapting to
induction tasks (e.g., close to random-guess on the
binary classiﬁcation task TabFact), reminiscent of
recent attempts in program synthesis and induc-
tion using PLMs (Austin et al., 2021). For GPT-3
and Codex, better zero-shot performances can be
expected by better prompt design.
4.4
Structured Knowledge Encoding
Structured knowledge encoding has been widely
explored (Bogin et al., 2019; Lin et al., 2019; Agar-
wal et al., 2020; Saxena et al., 2020; Yasunaga and
Liang, 2020; Yasunaga et al., 2022; and others de-
tailed in Section 2). We hope that UNIFIEDSKG
can promote systematic study of general structured
knowledge encoding. To this end, this part focuses
on the linearization of structured knowledge.
Does the order of user input, structured knowl-
edge, and context matter? To explore the effect
of the order of user input, structured knowledge,
and context, we rerun the single-task experiments
while switching the order of these components in
both the training and development set. Table 7
shows that placing the text before structured knowl-
edge (rs) is better than the opposite (sr), which is
consistent across SKG tasks. Our explanation is
that the position of the text is relatively ﬁxed in rs,


Spider
WikiSQL
TabFact
Linearization
40.23
59.21
58.77
Natural Language
38.59
63.16
58.56
Table 9: Converting structured knowledge into natural
language for low-resource learning. A large improve-
ment is observed on question answering (WikiSQL),
but not on text2SQL semantic parsing (Spider) and fact
veriﬁcation (TabFact).
helping the decoder to learn stable attention over
the text. Also, placing the context in between the
text and structured knowledge yields better results.
Is T5 sensitive to structured knowledge order-
ing? Order-insensitivity is common for most struc-
tured knowledge, e.g., permutation of columns in
a table preserves the meaning. To study this in-
sensitivity, we evaluate T5-large on a manipulated
development set where the order of schema (for
database), column (for table), or slots and values
(for ontology) is reversed. Table 8 shows that tasks
with cross-domain tables and databases are less
order-sensitive, while models are very sensitive to
the order of ontology. Other types of robustness
(e.g., robustness to cell values irrelevant to the an-
swer) remain an open question in UNIFIEDSKG.
Is it beneﬁcial to represent structured knowl-
edge as natural language? SKG data is not typi-
cally used to pretrain PLMs. Given ample training
data, PLMs adapt well to SKG tasks, as shown
in Table 2. However, under the low-resource set-
ting, converting structured data to natural language
might be helpful. For Spider, we use a shared tem-
plate to convert structured data to natural language.
For TabFact and WikiSQL, we randomly selected
236 tables shared by both datasets and manually
labeled templates to convert each row into a sen-
tence. Examples of the templates are shown in
Appendix I. These templates produce about 1000
samples for each task, divided into training and
test sets. We ﬁnd that, in WikiSQL, the conversion
to natural language stabilizes and accelerates the
training process. Table 9 shows that conversion
to natural language improves the performance on
WikiSQL, has no signiﬁcant inﬂuence on TabFact,
and slightly degrades the performance on Spider.
4.5
Human Evaluation for Generation Tasks
For each generation task, we randomly sample 100
development set samples and ask human annotators
to judge the correctness of each output, using a 0-1
score. Details are provided in Appendix D.5. Table
Metric
T5-base
T5-large
T5-3B
FeTaQA
BLEU
29.00
30.94
31.73
Human∗†
36.0%
51.3%
57.3%
DART
BLEU
50.62
51.72
50.38
Human
90.7%
91.7%
87.7%
ToTTo
BLEU
48.29
48.95
48.95
Human
78.7%
80.0%
81.3%
KVRET
BLEU
20.04
18.84
17.75
Human†
72.3%
66.3%
75.0%
SQL2Text
BLEC
93.69
93.35
92.71
Human∗
83.7%
90.3%
84.7%
Logic2Text
BLEC
92.15
92.88
91.69
Human†
77.2%
81.5%
84.2%
Table 10: Automatic metrics and human evaluation on
the development set of generation tasks.
∗p < 0.05
for “the rank-1 model is better than the rank-2 model”.
†p < 0.05 for “the rank-2 model is better than the rank-
3 model”. Automatic metrics do not always reﬂect hu-
man evaluation. Larger models are not always better.
10 shows that automatic metrics do not always re-
ﬂect human evaluation, calling for better automatic
metrics to truly reﬂect the model’s ability on gen-
eration tasks. Larger models are not always better,
and detailed error analysis is provided below.
4.6
Error Analysis
Error analysis based on output validity
Uncon-
strained decoding from PLMs may generate invalid
outputs. For semantic parsing, we divide wrong
outputs into invalid outputs (i.e., not executable
when the output is SQL, and not parse-able when
the output is s-expression or TOP-representation)
and valid but wrong answers. Figure 3 shows that,
for SQL semantic parsing, a large number of errors
are caused by invalid outputs, and the number of in-
valid outputs gradually decreases with the increase
of model size. This phenomenon is also observed
by Scholak et al. (2021), who used constrained de-
coding to improve the validity, largely improving
the parsing performance. For s-expression seman-
tic parsing, invalid outputs take up 30–50% of all
wrong outputs, and increasing the model size does
not reduce invalidity signiﬁcantly. For fact veriﬁ-
cation tasks, valid outputs are “entailed” and “re-
futed”. We observe that T5 always generates valid
outputs. For question answering, we do not include
the validity analysis since the validity check for an
answer is non-trivial and could be imprecise.
Error analysis for text generation tasks
For
generation tasks, we consider four types of errors:
missing information (required information is not


base
large
3b
Size
0
100
200
300
Count
Error type
Invalid
Valid-but-wrong
(a) Spider
base
large
3b
Size
0
50
100
Count
Error type
Invalid
Valid-but-wrong
(b) GrailQA
base
large
3b
Size
0
10
20
30
40
50
Proportion
Error type
Miss. Inf.
Contra.
Halluc.
Ungram.
(c) FeTaQA
base
large
3b
Size
0
10
20
30
Proportion
Error type
Miss. Inf.
Contra.
Halluc.
Ungram.
(d) ToTTo
Figure 3:
Error analysis. For semantic parsing, we
plot the number of invalid/valid-but-wrong predictions.
For generation, we plot the proportion of missing-
information/contradiction/hallucination/ungrammatical
errors among all predictions (one prediction may have
multiple errors). Full visualization is in Appendix B.
shown in the output), contradiction (the output is
contradictory to the input), 3) hallucination (the
output contains information that cannot be veriﬁed
by the input), and 4) ungrammatical. Figure 3
shows that the proportion of ungrammatical outputs
is generally less than 5%. Missing information and
contradiction are common errors made by T5, and
performance gains generally come from reducing
contradiction. Hallucination is not a common error
made by T5 except for the highlighted-table-to-text
task (ToTTo), where T5 tends to output information
of non-highlighted cell values.
Case study
We summarize some interesting ob-
servations about the model output (more in Ap-
pendix H). Compared with T5-base and T5-large,
T5-3B’s outputs for text generation tasks tend to be
more diverse and creative as shown in Appendix
H.2 and H.7. Also, T5-3B sometimes leverages do-
main knowledge to summarize facts in some tasks
such as DART (e.g., describing rating 5 out of 5 as
low), while the other two copy the original expres-
sions in the input, as shown in Appendix H.5 and
H.6. However, this ability puts T5-3B in the risk
of manipulating information and meaning of user
request as shown in Appendix H.3.2 and H.4.
5
Conclusions
In this paper, we propose the UNIFIEDSKG frame-
work to promote systematic research on struc-
tured knowledge grounding by unifying 21 SKG
tasks. Using UNIFIEDSKG as a benchmark, we
demonstrate that ﬁnetuning T5 on individual tasks
achieves state-of-the-art results on almost all 21
tasks. We show that multi-task preﬁx-tuning bene-
ﬁts most SKG tasks, largely improving the overall
performance. For structured knowledge encoding,
we ﬁnd that the effectiveness of encoding varia-
tions varies across tasks. Moreover, UNIFIEDSKG
is a challenging testbed for zero-shot and few-shot
learning, shown by the poor results of large PLMs.
6
Limitations
UNIFIEDSKG establishes a powerful and repro-
ducible starting point for SKG research. New mod-
els can be easily applied to diverse SKG tasks, and
new tasks can be easily framed based on our stan-
dardized abstraction. UNIFIEDSKG promotes a
systematic study on more general and robust ad-
vances in structured knowledge encoding, multi-
task learning, zero-shot learning, and few-shot
learning for SKG tasks. It also would be interest-
ing to explore general pretraining methods within
UNIFIEDSKG, which potentially beneﬁt all the uni-
ﬁed tasks. When the structured knowledge is too
large for GPU memory, we truncate them based on
heuristic rules, calling for future study on 1) incor-
porating retrieval component in SKG, 2) designing
sparse attention in T5 for structured knowledge or
other means to improve model efﬁciency.
UNIFIEDSKG currently provides the correct type
of structured knowledge for each task. However,
how a system searches for the correct structured
knowledge resources, takes appropriate action, and
integrates information and results from multiple
structured sources given a user request is still under-
explored, which are a prerequisite for building a
uniﬁed multi-purpose SKG system.
Since we select popular tasks from each task
family, we risk disproportionality in terms of the
data language, domain and population, and we ac-
tively welcome diverse, multi-lingual tasks to be
added into UNIFIEDSKG. Also, the error analysis
of SKG can more ﬁne-grained, and we hope our
ﬁndings can promote future work on systematically
studying and decomposing the behavior of PLMs
on SKG tasks. Furthermore, training and evalua-
tion data should reﬂect the intents and linguistic
phenomena in the real world (de Vries et al., 2020),
suggesting more realistic tasks to be added into
UNIFIEDSKG.


References
Oshin Agarwal, Heming Ge, Siamak Shakeri, and
Rami Al-Rfou. 2020. Knowledge graph based syn-
thetic corpus generation for knowledge-enhanced
language model pre-training.
arXiv preprint
arXiv:2010.12688.
Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava,
Xilun Chen, Luke Zettlemoyer, and Sonal Gupta.
2021a.
Muppet:
Massive multi-task representa-
tions with pre-ﬁnetuning. In Proceedings of EMNLP
2021, pages 5799–5811, Online and Punta Cana, Do-
minican Republic.
Armen Aghajanyan, Dmytro Okhonko, Mike Lewis,
Mandar Joshi, Hu Xu, Gargi Ghosh, and Luke Zettle-
moyer. 2021b. Htlm: Hyper-text pre-training and
prompting of language models.
Rami Aly, Zhijiang Guo, Michael Sejr Schlichtkrull,
James
Thorne,
Andreas
Vlachos,
Christos
Christodoulopoulos, Oana Cocarascu, and Arpit
Mittal. 2021.
The fact extraction and VERiﬁca-
tion over unstructured and structured information
(FEVEROUS) shared task.
In Proceedings of
the Fourth Workshop on Fact Extraction and
VERiﬁcation (FEVER), pages 1–13.
Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao,
Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Hon-
glei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni,
Jai Gupta, Kai Hui, Sebastian Ruder, and Donald
Metzler. 2021. Ext5: Towards extreme multi-task
scaling for transfer learning.
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten
Bosma, Henryk Michalewski, David Dohan, Ellen
Jiang, Carrie Cai, Michael Terry, Quoc V. Le, and
Charles Sutton. 2021. Program synthesis with large
language models. ArXiv, abs/2108.07732.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013.
Semantic parsing on freebase from
question-answer pairs.
In EMNLP 2013, pages
1533–1544.
Jonathan Berant and Percy Liang. 2014. Semantic pars-
ing via paraphrasing. In Proceedings of ACL 2014,
pages 1415–1425.
Ben Bogin, Matt Gardner, and Jonathan Berant. 2019.
Global reasoning over database structures for text-to-
sql parsing. In Proceedings of EMNLP 2019.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah,
Jared
D
Kaplan,
Prafulla
Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry,
Amanda Askell, Sandhini Agarwal, Ariel Herbert-
Voss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,
Clemens Winter, Chris Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language Models are Few-Shot Learners. In
Advances in Neural Information Processing Systems,
volume 33, pages 1877–1901.
Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang
Tseng, Iñigo Casanueva, Ultes Stefan, Ramadan Os-
man, and Milica Gaši´c. 2018. Multiwoz - a large-
scale multi-domain wizard-of-oz dataset for task-
oriented dialogue modelling. In Proceedings of the
2018 Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, Alex Ray, Raul Puri, Gretchen
Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-
try, Pamela Mishkin, Brooke Chan, Scott Gray,
Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz
Kaiser,
Mohammad Bavarian,
Clemens Winter,
Philippe Tillet, Felipe Petroski Such, Dave Cum-
mings, Matthias Plappert, Fotios Chantzis, Eliza-
beth Barnes, Ariel Herbert-Voss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N.
Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welin-
der, Bob McGrew, Dario Amodei, Sam McCandlish,
Ilya Sutskever, and Wojciech Zaremba. 2021a. Eval-
uating large language models trained on code.
Sanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao,
Ruofei Zhang, and Yangfeng Ji. 2021b. HittER: Hi-
erarchical transformers for knowledge graph embed-
dings. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing,
pages 10395–10407.
Wenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and
William Yang Wang. 2020a.
Logical natural lan-
guage generation from open-domain tables. In Pro-
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 7929–
7942, Online. Association for Computational Lin-
guistics.
Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai
Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and
William Yang Wang. 2020b. Tabfact : A large-scale
dataset for table-based fact veriﬁcation.
In Inter-
national Conference on Learning Representations
(ICLR), Addis Ababa, Ethiopia.
Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan
Xiong, Hong Wang, and William Wang. 2020c. Hy-
bridqa: A dataset of multi-hop question answering
over tabular and textual data. Findings of EMNLP
2020.
Zhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou
Zhou, Yunkai Zhang, Sairam Sundaresan, and
William Yang Wang. 2020d.
Logic2Text: High-
ﬁdelity natural language generation from logical
forms. In Findings of the Association for Compu-
tational Linguistics: EMNLP 2020.


Zimin Chen, Vincent Josua Hellendoorn, Pascal Lam-
blin, Petros Maniatis, Pierre-Antoine Manzagol,
Daniel Tarlow,
and Subhodeep Moitra. 2021c.
PLUR: A unifying, graph-based view of program
learning, understanding, and repair. In Thirty-Fifth
Conference on Neural Information Processing Sys-
tems.
Jordan Clive, Kris Cao, and Marek Rei. 2021. Control
preﬁxes for parameter-efﬁcient text generation.
Yinpei Dai, Hangyu Li, Yongbin Li, Jian Sun, Fei
Huang, Luo Si, and Xiaodan Zhu. 2021. Preview,
attend and review: Schema-aware curriculum learn-
ing for multi-domain dialogue state tracking. In Pro-
ceedings ACL-IJCNLP 2021 (Volume 2: Short Pa-
pers), pages 879–885, Online.
Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya
Godbole, Ethan Perez, Jay Yoon Lee, Lizhen
Tan, Lazaros Polymenakos, and Andrew McCallum.
2021.
Case-based reasoning for natural language
queries over knowledge bases.
In Proceedings of
EMNLP 2021, pages 9594–9611, Online and Punta
Cana, Dominican Republic.
Harm de Vries, Dzmitry Bahdanau, and Christopher D.
Manning. 2020.
Towards ecologically valid re-
search on language user interfaces. ArXiv.
Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju,
Rishabh Singh, Abdel rahman Mohamed, and Push-
meet Kohli. 2017. Robustﬁll: Neural program learn-
ing under noisy i/o. In ICML.
Julian Martin Eisenschlos, Maharshi Gor, Thomas
Müller, and William W Cohen. 2021. Mate: Multi-
view attention for table transformer efﬁciency. arXiv
preprint arXiv:2109.04312.
Mihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi,
Sanchit Agarwal, Shuyag Gao, and Dilek Hakkani-
Tur. 2019.
Multiwoz 2.1: Multi-domain dialogue
state corrections and state tracking baselines. arXiv
preprint arXiv:1907.01669.
Mihail Eric, Lakshmi. Krishnan, François Charette,
and Christopher D. Manning. 2017. Key-value re-
trieval networks for task-oriented dialogue. In SIG-
DIAL Conference.
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
Making pre-trained language models better few-shot
learners. In Association for Computational Linguis-
tics (ACL).
Yanjie Gou, Yinjie Lei, Lingqiao Liu, Yong Dai,
and Chunxu Shen. 2021. Contextualize knowledge
bases with transformer for end-to-end task-oriented
dialogue systems.
In Proceedings of the EMNLP
2021, pages 4300–4310, Online and Punta Cana, Do-
minican Republic.
Yu Gu, Sue Kase, Michelle Vanni, Brian Sadler, Percy
Liang, Xifeng Yan, and Yu Su. 2021. Beyond iid:
three levels of generalization for question answering
on knowledge bases. In Proceedings of the Web Con-
ference 2021.
Jonathan Herzig, P. Nowak, Thomas Müller, Francesco
Piccinno, and Julian Martin Eisenschlos. 2020.
Tapas:
Weakly supervised table parsing via pre-
training. In Proceedings of ACL.
Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu,
Semih Yavuz, and Richard Socher. 2020. A simple
language model for task-oriented dialogue. In Pro-
ceedings of Conference on Neural Information Pro-
cessing Systems (NeurIPS).
Binyuan Hui, Ruiying Geng, Qiyu Ren, Binhua Li,
Yongbin Li, Jian Sun, Fei Huang, Luo Si, Pengfei
Zhu, and Xiaodan Zhu. 2021. Dynamic hybrid re-
lation network for cross-domain context-dependent
semantic parsing.
Wonseok Hwang, Jinyeung Yim, Seunghyun Park, and
Minjoon Seo. 2019. A comprehensive exploration
on wikisql with table-aware word contextualization.
ArXiv, abs/1902.01069.
Hiroshi Iida, Dung Thai, Varun Manjunatha, and Mohit
Iyyer. 2021. Tabbie: Pretrained representations of
tabular data. arXiv preprint arXiv:2105.02584.
Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017.
Search-based neural structured learning for sequen-
tial question answering. In Proceedings of the 55th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
1821–1831, Vancouver, Canada.
Mihir Kale and Abhinav Rastogi. 2020. Text-to-text
pre-training for data-to-text tasks. In Proceedings of
INLG 2020, Dublin, Ireland, December 15-18, 2020,
pages 97–102.
D. Khashabi,
S. Min,
T. Khot,
A. Sabhwaral,
O. Tafjord, P. Clark, and H. Hajishirzi. 2020. Uni-
ﬁedqa: Crossing format boundaries with a single qa
system. EMNLP - ﬁndings.
Chia-Hsuan Lee, Hao Cheng, and Mari Ostendorf.
2021.
Dialogue state tracking with a language
model using schema-driven prompting. In Proceed-
ings of the 2021 Conference on Empirical Methods
in Natural Language Processing, pages 4937–4949.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efﬁcient prompt
tuning.
In Proceedings of EMNLP 2021, Virtual
Event / Punta Cana, Dominican Republic, 7-11
November, 2021, pages 3045–3059.
Quentin Lhoest, Albert Villanova del Moral, Yacine
Jernite, Abhishek Thakur, Patrick von Platen, Suraj
Patil, Julien Chaumond, Mariama Drame, Julien Plu,
Lewis Tunstall, Joe Davison, Mario Šaško, Gun-
jan Chhablani, Bhavitvya Malik, Simon Brandeis,
Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas
Patry, Angelina McMillan-Major, Philipp Schmid,


Sylvain Gugger, Clément Delangue, Théo Matus-
sière, Lysandre Debut, Stas Bekman, Pierric Cistac,
Thibault Goehringer, Victor Mustar, François Lagu-
nas, Alexander M. Rush, and Thomas Wolf. 2021.
Datasets: A community library for natural language
processing.
Haoran Li, Abhinav Arora, Shuohui Chen, Anchit
Gupta, Sonal Gupta, and Yashar Mehdad. 2021.
MTOP: A comprehensive multilingual task-oriented
semantic parsing benchmark. In Proceedings of the
16th Conference of the European Chapter of the As-
sociation for Computational Linguistics: Main Vol-
ume, pages 2950–2962, Online.
Xiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning:
Optimizing continuous prompts for generation. In
Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), pages
4582–4597, Online.
Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang
Ren. 2019.
Kagnet: Knowledge-aware graph net-
works for commonsense reasoning. In Proceedings
of EMNLP-IJCNLP.
Zhaojiang Lin, Bing Liu, Seungwhan Moon, Paul A
Crook, Zhenpeng Zhou, Zhiguang Wang, Zhou Yu,
Andrea Madotto, Eunjoon Cho, and Rajen Subba.
2021.
Leveraging slot descriptions for zero-shot
cross-domain dialogue statetracking.
In Proceed-
ings of NAACL 2021, pages 5640–5648.
Qian Liu, Bei Chen, Jiaqi Guo, Zeqi Lin, and Jian
guang Lou. 2021.
Tapex: Table pre-training via
learning a neural sql executor.
Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju,
Haotang Deng, and Ping Wang. 2020.
K-bert:
Enabling language representation with knowledge
graph. In AAAI.
Andrea Madotto, Chien-Sheng Wu, and Pascale Fung.
2018. Mem2seq: Effectively incorporating knowl-
edge bases into end-to-end task-oriented dialog sys-
tems. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 1468–1478.
Alana Marzoev, Samuel Madden, M Frans Kaashoek,
Michael Cafarella, and Jacob Andreas. 2020. Unnat-
ural language processing: Bridging the gap between
synthetic and natural language data. arXiv preprint
arXiv:2004.13645.
Bryan McCann, Nitish Shirish Keskar, Caiming Xiong,
and Richard Socher. 2018. The natural language de-
cathlon: Multitask learning as question answering.
CoRR, abs/1806.08730.
Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Vic-
toria Lin, Neha Verma, Rui Zhang, Wojciech Kry´s-
ci´nski, Nick Schoelkopf, Riley Kong, Xiangru Tang,
Murori Mutuma, Ben Rosand, Isabel Trindade,
Renusree Bandaru, Jacob Cunningham, Caiming
Xiong, and Dragomir Radev. 2021a. Fetaqa: Free-
form table question answering. TACL.
Linyong Nan, Dragomir Radev, Rui Zhang, Amrit
Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xian-
gru Tang, Aadit Vyas, Neha Verma, Pranav Kr-
ishna, Yangxiaokang Liu, Nadia Irwanto, Jessica
Pan, Faiaz Rahman, Ahmad Zaidi, Murori Mutuma,
Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan,
Xi Victoria Lin, Caiming Xiong, Richard Socher,
and Nazneen Fatema Rajani. 2021b.
Dart: Open-
domain structured data record to text generation. In
NAACL.
Jekaterina Novikova, Ondrej Dusek, and Verena Rieser.
2017. The E2E dataset: New challenges for end-to-
end generation. In SIGDial 2017, pages 201–206.
Barlas
Oguz,
Xilun
Chen,
Vladimir
Karpukhin,
Stan
Peshterliev,
Dmytro
Okhonko,
Michael
Schlichtkrull, Sonal Gupta, Yashar Mehdad, and
Scott Yih. 2021. Unik-qa: Uniﬁed representations
of
structured
and
unstructured
knowledge
for
open-domain question answering.
arXiv preprint
arXiv:2012.14610.
Ankur P Parikh, Xuezhi Wang, Sebastian Gehrmann,
Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and
Dipanjan Das. 2020. ToTTo: A controlled table-to-
text generation dataset. In Proceedings of EMNLP.
Panupong Pasupat and Percy Liang. 2015. Composi-
tional semantic parsing on semi-structured tables. In
Proceedings of the 53rd Annual Meeting of the Asso-
ciation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (Volume 1: Long Papers), pages 1470–
1480, Beijing, China.
Panupong Pasupat, Yuan Zhang, and Kelvin Guu. 2021.
Controllable semantic parsing via retrieval augmen-
tation. In Proceedings of EMNLP 2021, pages 7683–
7698, Online and Punta Cana, Dominican Republic.
Matt Post. 2018. A call for clarity in reporting BLEU
scores. In Proceedings of the Third Conference on
Machine Translation: Research Papers, pages 186–
191, Belgium, Brussels.
Libo Qin, Xiao Xu, Wanxiang Che, Yue Zhang, and
Ting Liu. 2020. Dynamic fusion network for multi-
domain end-to-end task-oriented dialog.
In Pro-
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 6344–
6354, Online.
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2020. Exploring the lim-
its of transfer learning with a uniﬁed text-to-text
transformer. Journal of Machine Learning Research,
21(140):1–67.


Nils Reimers and Iryna Gurevych. 2020.
Making
monolingual sentence embeddings multilingual us-
ing knowledge distillation.
In Proceedings of
EMNLP 2020.
Hongyu Ren, Hanjun Dai, Bo Dai, Xinyun Chen,
Michihiro Yasunaga, Haitian Sun, Dale Schuurmans,
Jure Leskovec, and Denny Zhou. 2021. Lego: La-
tent execution-guided reasoning for multi-hop ques-
tion answering on knowledge graphs.
In Interna-
tional Conference on Machine Learning (ICML).
Ohad Rubin and Jonathan Berant. 2021. SmBoP: Semi-
autoregressive bottom-up semantic parsing. In Pro-
ceedings of NAACL 2021, pages 311–324, Online.
Victor Sanh, Albert Webson, Colin Raffel, Stephen H.
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chafﬁn, Arnaud Stiegler, Teven Le Scao, Arun Raja,
Manan Dey, M Saiful Bari, Canwen Xu, Urmish
Thakker, Shanya Sharma Sharma, Eliza Szczechla,
Taewoon Kim, Gunjan Chhablani, Nihal Nayak,
Debajyoti Datta, Jonathan Chang, Mike Tian-Jian
Jiang, Han Wang, Matteo Manica, Sheng Shen,
Zheng Xin Yong, Harshit Pandey, Rachel Bawden,
Thomas Wang, Trishala Neeraj, Jos Rozen, Ab-
heesht Sharma, Andrea Santilli, Thibault Fevry, Ja-
son Alan Fries, Ryan Teehan, Stella Biderman, Leo
Gao, Tali Bers, Thomas Wolf, and Alexander M.
Rush. 2021.
Multitask prompted training enables
zero-shot task generalization.
Apoorv Saxena, Aditay Tripathi, and Partha Taluk-
dar. 2020.
Improving multi-hop question answer-
ing over knowledge graphs using knowledge base
embeddings. In Association for Computational Lin-
guistics (ACL).
Torsten Scholak, Nathan Schucher, and Dzmitry Bah-
danau. 2021. PICARD: Parsing incrementally for
constrained auto-regressive decoding from language
models.
In Proceedings of EMNLP 2021, pages
9895–9901.
Peter Shaw, Ming-Wei Chang, Panupong Pasupat, and
Kristina Toutanova. 2021. Compositional general-
ization and natural language variation: Can a seman-
tic parsing approach handle both? In ACL/IJCNLP.
Richard Shin, Christopher H Lin, Sam Thomson,
Charles Chen, Subhro Roy, Emmanouil Antonios
Platanios, Adam Pauls, Dan Klein, Jason Eisner, and
Benjamin Van Durme. 2021. Constrained language
models yield few-shot semantic parsers.
arXiv
preprint arXiv:2104.08768.
Chang Shu, Yusen Zhang, Xiangyu Dong, Peng Shi,
Tao Yu, and Rui Zhang. 2021.
Logic-consistency
text generation from semantic parses. In Findings of
the Association for Computational Linguistics: ACL-
IJCNLP 2021.
Yixuan Su, David Vandyke, Sihui Wang, Yimai Fang,
and Nigel Collier. 2021. Plan-then-generate: Con-
trolled data-to-text generation via planning.
In
EMNLP.
A. Talmor and J. Berant. 2018.
The web as a
knowledge-base for answering complex questions.
In North American Association for Computational
Linguistics (NAACL).
Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav,
Yizhong Wang, Akari Asai, Gabriel Ilharco, Han-
naneh Hajishirzi, and Jonathan Berant. 2021. Mul-
timodal{qa}: complex question answering over text,
tables and images. In International Conference on
Learning Representations.
Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou,
and Daniel Cer. 2021. Spot: Better frozen model
adaptation through soft prompt transfer.
Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr
Polozov, and Matthew Richardson. 2020. Rat-sql:
Relation-aware schema encoding and linking for
text-to-sql parsers. In ACL.
Bailin Wang, Ivan Titov, and Mirella Lapata. 2019.
Learning semantic parsers from denotations with la-
tent structured alignments and abstract programs. In
Proceedings of EMNLP-IJCNLP 2019, pages 3774–
3785, Hong Kong, China.
Quan Wang, Zhendong Mao, Bin Wang, and Li Guo.
2017. Knowledge graph embedding: A survey of
approaches and applications.
IEEE Transactions
on Knowledge and Data Engineering, 29(12):2724–
2743.
Sinong Wang, Han Fang, Madian Khabsa, Hanzi Mao,
and Hao Ma. 2021a. Entailment as few-shot learner.
CoRR, abs/2104.14690.
Zhiruo Wang, Haoyu Dong, Ran Jia, Jia Li, Zhiyi Fu,
Shi Han, and Dongmei Zhang. 2021b. Tuta: Tree-
based transformers for generally structured table pre-
training. In Proceedings of the 27th ACM SIGKDD
Conference on Knowledge Discovery & Data Min-
ing, pages 1780–1790.
Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M. Dai, and Quoc V. Le. 2021. Finetuned lan-
guage models are zero-shot learners. arXiv preprint.
Jason D Williams, Antoine Raux, and Matthew Hen-
derson. 2016. The dialog state tracking challenge
series: A review. Dialogue & Discourse, 7(3):4–33.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. 2020.
Transformers: State-of-the-art natural language pro-
cessing. In Proceedings of EMNLP 2020: System
Demonstrations, pages 38–45, Online.


Chien-Sheng Wu, Richard Socher, and Caiming Xiong.
2019. Global-to-local memory pointer networks for
task-oriented dialogue. In Proceedings of the Inter-
national Conference on Learning Representations
(ICLR).
Xiaoyu Yang, Feng Nie, Yufei Feng, Quan Liu, Zhi-
gang Chen, and Xiaodan Zhu. 2020. Program en-
hanced fact veriﬁcation with verbalization and graph
attention network. In Proceedings of EMNLP 2020,
pages 7810–7825, Online.
Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren,
Xikun Zhang, Christopher D. Manning, Percy Liang,
and Jure Leskovec. 2022.
Deep bidirectional
language-knowledge graph pretraining.
In Neural
Information Processing Systems (NeurIPS).
Michihiro Yasunaga and Percy Liang. 2020.
Graph-
based, self-supervised program repair from diagnos-
tic feedback.
In International Conference on Ma-
chine Learning (ICML).
Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut,
Percy Liang, and Jure Leskovec. 2021. QA-GNN:
Reasoning with language models and knowledge
graphs for question answering. In North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (NAACL).
Association for Computational Linguistics.
Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021a.
Crossﬁt: A few-shot learning challenge for cross-
task generalization in nlp.
In Proceedings of
EMNLP.
Xi Ye, Semih Yavuz, Kazuma Hashimoto, Yingbo
Zhou, and Caiming Xiong. 2021b.
Rng-kbqa:
Generation augmented iterative ranking for knowl-
edge base question answering.
arXiv preprint
arXiv:2109.08678.
Wen-tau Yih, Matthew Richardson, Chris Meek, Ming-
Wei Chang, and Jina Suh. 2016. The value of se-
mantic parse labeling for knowledge base question
answering. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers).
Pengcheng Yin and Graham Neubig. 2018. TRANX:
A transition-based neural abstract syntax parser for
semantic parsing and code generation. In EMNLP
2018: System Demonstrations, pages 7–12.
Pengcheng Yin, Graham Neubig, Wen-tau Yih, and
Sebastian Riedel. 2020a. TaBERT: Pretraining for
joint understanding of textual and tabular data. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics. Association
for Computational Linguistics.
Wenpeng Yin, Nazneen Fatema Rajani, Dragomir R.
Radev, Richard Socher, and Caiming Xiong. 2020b.
Universal natural language processing with limited
annotations: Try few-shot textual entailment as a
start.
In Proceedings of EMNLP 2020, Online,
November 16-20, 2020, pages 8229–8239.
Ori
Yoran,
Alon
Talmor,
and
Jonathan
Berant.
2021.
Turning tables:
Generating examples
from semi-structured tables for endowing language
models with reasoning skills.
arXiv preprint
arXiv:2107.07261.
Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue,
Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze
Shi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga,
Sungrok Shim, Tao Chen, Alexander Fabbri, Zifan
Li, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vin-
cent Zhang, Caiming Xiong, Richard Socher, Walter
Lasecki, and Dragomir Radev. 2019a. CoSQL: A
conversational text-to-SQL challenge towards cross-
domain natural language interfaces to databases. In
Proceedings of EMNLP 2019, pages 1962–1979,
Hong Kong, China.
Tao Yu, Rui Zhang, Oleksandr Polozov, Christo-
pher Meek, and Ahmed Hassan Awadallah. 2021.
SCoRE: Pre-training for context representation in
conversational semantic parsing.
In International
Conference on Learning Representations.
Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,
Dongxu Wang, Zifan Li, James Ma, Irene Li,
Qingning Yao,
Shanelle Roman,
Zilin Zhang,
and Dragomir Radev. 2018.
Spider:
A large-
scale human-labeled dataset for complex and cross-
domain semantic parsing and text-to-sql task.
In
Proceedings of EMNLP 2018, Brussels, Belgium.
Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern
Tan, Xi Victoria Lin, Suyi Li, Irene Li Heyang Er,
Bo Pang,
Tao Chen,
Emily Ji,
Shreya Dixit,
David Proctor,
Sungrok Shim,
Vincent Zhang
Jonathan Kraft, Caiming Xiong, Richard Socher,
and Dragomir Radev. 2019b. Sparc: Cross-domain
semantic parsing in context. In Proceedings of the
57th Annual Meeting of the Association for Compu-
tational Linguistics, Florence, Italy.
Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago On-
tanon, Philip Pham, Anirudh Ravula, Qifan Wang,
Li Yang, and Amr Ahmed. 2020. Big Bird: Trans-
formers for Longer Sequences.
In Advances in
Neural Information Processing Systems, volume 33,
pages 17283–17297.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In AAAI 1996, pages 1050–1055.
Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured
classiﬁcation with probabilistic categorial grammars.
UAI.
Hongzhi Zhang, Yingyao Wang, Sirui Wang, Xuezhi
Cao, Fuzheng Zhang, and Zhongyuan Wang. 2020.
Table fact veriﬁcation with structure-aware trans-
former.
In Proceedings of EMNLP 2020, pages
1624–1629.


Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein.
2021.
Adapting language models for zero-shot
learning by meta-tuning on dataset and prompt col-
lections. In Findings of EMNLP.
Ruiqi Zhong, Tao Yu, and Dan Klein. 2020. Semantic
evaluation for text-to-sql with distilled test suite. In
EMNLP 2020.
Victor Zhong, Caiming Xiong, and Richard Socher.
2017.
Seq2sql:
Generating structured queries
from natural language using reinforcement learning.
CoRR, abs/1709.00103.


A
Contributions
Code implementation
Tianbao Xie and Chen
Henry Wu implemented the code base of the UNI-
FIEDSKG framework and experiment pipeline. The
code of PICARD and advice from Torsten Scholak
sped up the implementation.
Task uniﬁcation
Tianbao Xie, Peng Shi, Michi-
hiro Yasunaga, Chen Henry Wu, and Ming Zhong
implemented the 21 tasks into the text-to-text for-
mat, adapted the metrics, and veriﬁed the perfor-
mances.
Paper writing
Chen Henry Wu and Tianbao
Xie ﬁnished most part of the paper. Michihiro
Yasunaga, Peng Shi, and Chengzu Li added re-
sults and analysis for their corresponding parts.
Peng Shi drafted related work on SKG with PLMs.
Torsten Scholak, Pengcheng Yin, Rui Zhang, Ruiqi
Zhong, Victor Zhong, Michihiro Yasunaga, Connor
Boyle, Chien-Sheng Wu, Sida Wang, Bailin Wang,
Ansong Ni, Ziyu Yao, Lingpeng Kong, Caiming
Xiong, Dragomir Radev, Noah A. Smith, and Luke
Zettlemoyer carefully reviewed the paper and gave
feedback for multiple rounds.
Experiments
Chen Henry Wu, Tianbao Xie,
and Chien-Sheng Wu conducted experiments on
individual tasks and multi-task learning.
Tian-
bao conducted the zero-shot learning experiments.
Chengzu Li and Tianbao Xie conducted the few-
shot learning experiments. Tianbao Xie conducted
experiments on the ordering of sequence inputs and
order-sensitivity. Chengzu Li, Connor Boyle, and
Peng Shi conducted the experiments on converting
structured knowledge into natural language.
Human evaluation
Chen Henry Wu organized
the human evaluation. Torsten Scholak, Rui Zhang,
Chengzu Li, Connor Boyle, Tianbao Xie, Peng
Shi, Tao Yu, and Chen Henry Wu were the human
participants.
Error analysis and case study
Tianbao Xie,
Chen Henry Wu, and Michihiro Yasunaga designed
and conducted the error analysis for semantic pars-
ing and generation tasks. Authors who participated
in the human annotation selected the cases for case
study.
Discussion
We had three separate weekly meet-
ings, and everyone in the project attended one of
them. Torsten Scholak, Ruiqi Zhong, Pengcheng
Yin, Victor Zhong, Peng Shi, Rui Zhang, Sida
Wang, and Lingpeng Kong actively provided ad-
vice. Torsten Scholak provided signals that preﬁx-
tuning would be comparable to ﬁne-tuning. Ruiqi
Zhong gave advice on analyzing the effect of model
size, Pengcheng Yin and Peng Shi gave advice on
analysis on converting structured knowledge into
natural language. Pengcheng Yin helped interpret
experimental results. Ziyu Yao suggested that we
report both sota (w/ extra) and sota (w/o extra) for
a fair comparison. Victor Zhong and Bailin Wang
gave valuable suggestions on multi-task learning
and task transfer analysis. Luke Zettlemoyer, Noah
A. Smith, Caiming Xiong, and Dragomir Radev
gave valuable comments on research questions and
experimental design.
Computing resources
We thank Salesforce Re-
search, an Amazon Research Award, ServiceNow
Research, and Yale NLP for providing computing
resources generously.
Tao Yu designed and led the research.
Acknowledgments
We thank Yifei Min and Libo Qin for their early-
stage discussion. We thank Panupong Pasupat and
William W. Cohen for their valuable feedback on
our initial draft. We thank Qian Liu for his TAPEX
code and advice on question answering tasks. We
thank wandb for free logging and OpenAI for free
Codex usage.


B
Results with Full Metrics
Metric
T5-base
T5-large
T5-3B
Spider
Match
58.121.46
66.632.31
71.76
Exec
60.060.54
68.281.61
74.37
Test suite
56.220.73
64.121.28
68.38
GrailQA
Match
60.00
67.00
69.00
WebQSP
F1
72.50
73.96
75.97
MTOP
Match
83.89
84.70
84.88
Template
88.85
88.32
88.86
WikiTQ
Acc
36.940.19
43.300.25
50.65
WikiSQL
Acc
84.50
86.27
87.34
CompWebQ
Acc
66.71
68.85
70.27
F1
80.02
81.05
81.43
Hits@1
83.64
85.49
86.20
HybridQA
Acc
54.07
56.95
59.41
F1
61.85
64.62
66.76
MMQA
Acc
67.29
74.08
78.48
F1
75.51
81.84
82.28
FeTaQA
BLEU
29.00
30.94
31.73
DART
BLEU
50.620.72
51.720.15
50.38
ToTTo
BLEU
48.29
48.95
48.95
MultiWoZ2.1
Joint Acc
57.520.96
58.230.39
58.46
KVRET
BLEU
20.04
18.84
17.75
SParC
Match
50.54
56.69
61.51
Exec
53.95
60.60
67.33
Match (interact)
31.28
37.44
41.94
Exec (interact)
34.36
41.23
46.45
CoSQL
Match
42.30
48.26
54.08
Exec
49.26
56.01
62.23
Match (interact)
12.63
16.72
22.78
Exec (interact)
16.04
20.14
26.16
SQA
Overall Acc
49.49
59.12
60.93
TabFact
Acc
76.340.36
81.400.16
83.97
FEVEROUS
Acc
75.05
79.81
82.40
SQL2Text
BLEC
93.690.29
93.350.29
92.71
Logic2Text
BLEC
92.15
92.88
91.69
Table 11: Development set performance with full met-
rics. We do three experiments with different random
seeds on representative task of each family and report
their averages and standard variances format as avrvar.
For the KVRET dataset, instead of the version
used in our main tables, we re-run another more
widely used pre-processed version (Madotto et al.,
2018; Wu et al., 2019; Qin et al., 2020) on T5-base,
T5-large and T5-3b. Results are shown in Table 13.
C
Input and Output Length Analysis
Linearization of large structured knowledge input
(e.g., large tables and KGs) can be arbitrarily long,
which needs to be truncated to ﬁt in GPUs with a
Metric
T5-base
T5-large
T5-3B
GrailQA
Match
62.39
67.30
70.11
WebQSP
F1
78.83
79.45
80.70
MTOP
Match
85.49
86.17
86.78
Template
87.52
89.53
90.20
WikiTQ
Acc
35.760.66
43.220.65
49.29
WikiSQL
Acc
82.63
84.80
85.96
CompWebQ
Acc
68.43
71.38
73.26
F1
80.20
81.76
82.58
Hits@1
83.70
85.40
86.08
FeTaQA
BLEU
29.91
32.45
33.44
ROUGE-1-Fmeasure
61.77
64.01
65.21
ROUGE-2-Fmeasure
39.44
42.26
43.09
ROUGE-L-Fmeasure
51.93
54.29
55.31
METEOR
48.53
50.80
51.23
BertScore-F1
0.92
0.93
0.93
BLEURT
-0.01
0.06
0.09
DART
BLEU
46.220.66
46.890.53
46.66
TER
61.800.20
60.970.31
60.70
METEOR
55.090.35
55.760.25
55.67
BertScore-F1
0.950.00
0.950.00
0.95
BLEURT
0.28330.0057
0.300.00
0.30
MultiWoZ2.1
Joint Acc
54.640.22
54.450.20
55.42
KVRET
BLEU
17.41
17.27
15.45
F1 micro all
66.45
65.85
67.88
F1 micro schedule
73.48
75.90
77.99
F1 micro navigate
64.89
62.72
65.47
F1 micro weather
63.78
62.80
64.01
SQA
Overall Acc
52.91
61.28
62.37
Pos 0 Acc
62.93
67.80
59.51
Pos 1 Acc
44.43
55.08
60.25
Pos 2 Acc
50.44
61.88
68.77
Pos 3 Acc
53.71
58.08
65.07
Interaction Acc
22.24
32.59
33.17
TabFact
All Acc
76.130.39
80.850.24
83.68
Simple Acc
-
91.380.32
93.10
Complex Acc
-
75.760.19
79.12
Small Acc
-
82.610.32
85.39
SQL2Text
BLEC
93.521.00
93.681.12
94.78
Logic2Text
BLEC
90.66
90.57
91.39
Table 12: Test set performance with full metrics (for
tasks with a publicly available test set). We do three
experiments with different random seeds on representa-
tive task of each family and report their averages and
standard variances format as avrvar.
limited size. The input and output are tokenized
by T5Tokenizer in Huggingface’s Transformers.3
We visualize the length distribution in Figure 5,
and details are presented in Table 14. Among the
datasets with very long inputs, we choose Wik-
iTableQuestion to study the impact of input length.
We visualize the table length distribution and per-
formances with different input truncation lengths in
Figure 6. We observe that the accuracy increases as
the input becomes longer, motivating future work to
study how to effectively encode large structured in-
put, e.g., leveraging sparse attention (Zaheer et al.,
2020).
3https://huggingface.co/t5-base/tree/main


base
large
3b
Size
0
100
200
300
Count
Error type
Invalid
Valid-but-wrong
(a) Spider
base
large
3b
Size
0
100
200
300
Count
Error type
Invalid
Valid-but-wrong
(b) CoSQL
base
large
3b
Size
0
100
200
300
Count
Error type
Invalid
Valid-but-wrong
(c) SParC
base
large
3b
Size
0
50
100
Count
Error type
Invalid
Valid-but-wrong
(d) GrailQA
base
large
3b
Size
0
50
100
150
Count
Error type
Invalid
Valid-but-wrong
(e) WebQSP
base
large
3b
Size
0
100
200
300
400
Count
Error type
Invalid
Valid-but-wrong
(f) MTOP
base
large
3b
Size
0
10
20
30
40
50
Proportion
Error type
Miss. Inf.
Contra.
Halluc.
Ungram.
(g) FeTaQA
base
large
3b
Size
0
5
10
15
20
Proportion
Error type
Miss. Inf.
Contra.
Halluc.
Ungram.
(h) DART
base
large
3b
Size
0
10
20
30
Proportion
Error type
Miss. Inf.
Contra.
Halluc.
Ungram.
(i) ToTTo
base
large
3b
Size
0
10
20
30
Proportion
Error type
Miss. Inf.
Contra.
Halluc.
Ungram.
(j) KVRET
base
large
3b
Size
0
5
10
15
20
Proportion
Error type
Miss. Inf.
Contra.
Halluc.
Ungram.
(k) SQL2Text
base
large
3b
Size
0
10
20
Proportion
Error type
Miss. Inf.
Contra.
Halluc.
Ungram.
(l) Logic2Text
Figure 4:
Error analysis. For semantic parsing, we show the number of invalid/valid-but-wrong predictions.
For generation tasks, we show the proportion of missing-information/contradiction/hallucination/ungrammatical
predictions among all predictions (one prediction may have multiple errors).
Metric
T5-base
T5-large
T5-3B
BLEU(dev)
22.80
23.07
22.71
BLEU(test)
21.21
22.36
20.40
F1 micro all(test)
67.49
68.03
70.07
F1 micro schedule(test)
79.39
79.47
78.54
F1 micro navigate(test)
62.87
63.59
65.34
F1 micro weather(test)
61.43
62.61
66.74
F1 macro all(test)
65.91
64.87
66.07
F1 macro schedule(test)
78.73
77.23
76.02
F1 macro navigate(test)
59.53
58.99
60.47
F1 macro weather(test)
64.05
62.58
65.78
Table 13: Baselines results are higher in pre-processed
KVRET dataset. It doesn’t change our conclusion on
T5 with simple modiﬁcation when necessary achieves
sota on almost all tasks.
D
Experimental Setup
D.1
Implementation Details
We use T5 (Raffel et al., 2020) as our backbone
language model. Each experiment For T5-3B ex-
periments, we use Deepspeed4 to save memory.
We use batch size 32 as default, except WikiTQ,
4https://github.com/microsoft/DeepSpeed
0
1000
2000
3000
4000
input tokens(structure input + request input)
0.000
0.005
0.010
0.015
0.020
0.025
Density
compwebq
cosql
dart
fetaqa
feverous
grailqa
hybridqa
kvret
logic2text
mmqa
multiwoz
ottqa
sparc
spider
sqa
sql2text
tab_fact
totto
webqsp
wikisql
wikitq
Figure 5: Input token distribution(<4096) in train set
from different tasks. We exclude MTOP since it con-
centrates on a relatively small ﬁeld which would make
this ﬁgure unreadable. In general, 1024 is a good length
for practice, and for most tasks, 2048 can hold all its in-
puts.
WikiSQL, and TabFact, for which for use batch
size 128 because we found it to work signiﬁcantly
better. We use the Adafactor optimizer for T5-base
and T5-large, and AdamW for T5-3b. We evaluate
on the development set for each 500 steps and use
the average development set metric for best check-


Structure Input Tokens
Text Input Tokens
Structure Input + Text Input Tokens
Sequence Output Tokens
Distribution(%)
[0, 512)
[512, 1024)
[1024,
8
)
[0, 512)
[512, 1024)
[1024,
8
)
[0, 512)
[512, 1024)
[1024,
8
)
[0, 128)
[128, 256)
[256,
8
)
Spider
97.01
1.81
1.17
100.00
0.00
0.00
95.47
3.35
1.17
98.81
1.18
0.0
GRAILQA
100.00
0.00
0.00
100.00
0.00
0.00
99.96
0.04
0.00
99.97
0.03
0.00
WebQsp
3.40
2.32
94.28
100.00
0.00
0.00
3.18
2.47
94.35
99.81
0.19
0.00
MTOP
0.00
100.00
0.00
100.00
0.00
0.00
0.00
100.00
0.00
99.97
0.03
0.00
WikiTableQuestions
48.32
27.48
24.18
100.00
0.00
0.00
46.03
29.43
24.52
99.98
0.01
0.01
WikiSQL
63.38
25.33
11.29
100.00
0.00
0.00
61.50
26.79
11.70
99.97
0.02
0.01
ComWebQ
1.18
14.52
84.30
100.00
0.00
0.00
1.09
11.28
87.63
99.59
0.39
0.01
HybridQA
35.53
50.63
13.8
100.00
0.00
0.00
31.77
53.35
14.86
100.00
0.00
0.0
MultiModalQA
63.02
25.67
11.30
100.00
0.00
0.00
60.54
27.26
12.18
99.99
0.01
0.00
FeTaQA
60.36
28.62
11.01
100.00
0.00
0.00
58.46
29.85
11.68
100.00
0.00
0.0
DART
100.00
0.00
0.00
100.00
0.00
0.00
100.00
0.00
0.00
99.99
0.01
0.0
ToTTo
95.80
2.87
1.31
100.00
0.00
0.00
95.80
2.87
1.31
99.99
0.01
0.0
MultiWoZ
100.00
0.00
0.00
98.77
1.21
0.01
54.76
45.09
0.13
0.00
100.00
0.0
KVRET
65.08
34.91
0.00
100.00
0.00
0.00
65.08
34.91
0.00
99.97
0.03
0.0
SParC
96.70
2.02
1.28
100.00
0.00
0.00
95.10
3.62
1.28
99.34
0.66
0.00
CoSQL
96.03
2.23
1.73
100.00
0.00
0.00
93.98
4.28
1.73
99.06
0.93
0.0
SQA
64.54
29.74
5.71
100.00
0.00
0.00
60.96
33.11
5.92
95.12
4.19
0.67
TabFact
63.22
28.19
8.58
100.00
0.00
0.00
60.68
30.20
9.10
100.00
0.00
0.0
FEVEROUS
61.37
22.24
16.39
100.00
0.00
0.00
57.53
25.07
17.40
100.00
0.00
0.00
SQL2Text
100.00
0.00
0.00
100.00
0.00
0.00
100.00
0.00
0.0
100.00
0.00
0.0
Logic2Text
100.00
0.00
0.00
100.00
0.00
0.00
100.00
0.00
0.0
100.00
0.00
0.0
Table 14: Input and output length for each task’s train set.
Structure Input Tokens
Text Input Tokens
Structure Input + Text Input Tokens
Sequence Output Tokens
Distribution(%)
[0, 512)
[512, 1024)
[1024,
8
)
[0, 512)
[512, 1024)
[1024,
8
)
[0, 512)
[512, 1024)
[1024,
8
)
[0, 128)
[128, 256)
[256,
8
)
Spider
100.00
0.00
0.00
100.00
0.00
0.00
100.00
0.00
0.00
99.23
0.77
0.00
GRAILQA
100.00
0.00
0.00
100.00
0.00
0.00
100.00
0.00
0.00
100.00
0.00
0.00
WebQsp
3.56
1.29
95.15
100.00
0.00
0.00
3.56
1.29
95.15
99.68
0.32
0.00
Russ
100.00
0.00
0.00
100.00
0.00
0.00
100.00
0.00
0.00
100.00
0.00
0.00
MTOP
0.00
100.00
0.00
100.00
0.00
0.00
0.00
100.00
0.00
100.00
0.00
0.00
WikiTableQuestions
49.56
28.65
21.79
100.00
0.00
0.00
48.60
29.11
22.29
99.93
0.07
0.00
WikiSQL
63.90
25.88
10.22
100.00
0.00
0.00
62.06
26.99
10.95
100.00
0.00
0.00
ComWebQ
0.28
15.79
83.93
100.00
0.00
0.00
0.28
12.66
87.06
99.00
1.00
0.00
HybridQA
38.37
52.63
9.00
100.00
0.00
0.00
34.16
56.00
9.84
100.00
0.00
0.00
MultiModalQA
66.22
25.72
8.06
100.00
0.00
0.00
64.02
27.38
8.59
100.00
0.00
0.00
FeTaQA
67.03
27.47
5.49
100.00
0.00
0.00
64.84
29.57
5.59
100.00
0.00
0.00
DART
100.00
0.00
0.00
100.00
0.00
0.00
100.00
0.00
0.00
100.00
0.00
0.00
ToTTo
95.82
2.92
1.26
100.00
0.00
0.00
95.82
2.92
1.26
100.00
0.00
0.00
MultiWoZ
100.00
0.00
0.00
99.16
0.84
0.00
25.07
74.68
0.24
0.00
100.00
0.00
KVRET
65.76
34.24
0.00
100.00
0.00
0.00
65.76
34.24
0.00
99.79
0.21
0.00
SParC
100.00
0.00
0.00
100.00
0.00
0.00
100.00
0.00
0.00
99.26
0.74
0.00
CoSQL
100.00
0.00
0.00
100.00
0.00
0.00
99.62
0.38
0.00
99.23
0.77
0.00
SQA
60.09
33.38
6.53
100.00
0.00
0.00
56.91
36.42
6.67
94.17
5.39
0.44
TabFact
62.17
29.31
8.52
100.00
0.00
0.00
59.95
30.91
9.14
100.00
0.00
0.00
FEVEROUS
61.56
23.71
14.73
100.00
0.00
0.00
57.57
26.58
15.85
100.00
0.00
0.00
SQL2Text
100.00
0.00
0.00
100.00
0.00
0.00
100.00
0.00
0.00
100.00
0.00
0.00
Logic2Text
100.00
0.00
0.00
100.00
0.00
0.00
100.00
0.00
0.00
100.00
0.00
0.00
Table 15: Input and output length for each task’s development set.
point selection. For all tasks, we set learning rate
to 5e-5 and used linear learning rate decay. All
experiments are done on NVIDIA Tesla V100 and
NVIDIA Tesla A100.
D.2
Metric Details
For most semantic parsing tasks, we report the ex-
act match accuracy of logical forms, and for task
has test suite (Zhong et al., 2020), we add test suite
metric to represent model’s performance; an ex-
ception is WebQSP, for which we follow previous
work to execute the parses and report the F1 score.
For QA, we report the exact match accuracy of
answer sets. For data-to-text generation, we re-
port sacre-BLEU (Post, 2018).5 We use each task’s
representative metric used by previous works. For
fact veriﬁcation, we report the accuracy. For high-
ﬁdelity NLG, we report BLEC (Shu et al., 2021),
which is the exact match between keywords in the
formal language and the natural language. Unless
speciﬁed, we use T5-large and report the develop-
ment set performance.
D.3
T0 Zero-shot Experimental Details
For each task in UNIFIEDSKG we search Sanh et al.
(2021) for the most similar instructions(if there is
no one for use, we create one follow their writing
5Signature: BLEU + case.lc + numrefs.1 + smooth.exp +
tok.13a + version.1.4.0


Structure Input Tokens
Text Input Tokens
Structure Input + Text Input Tokens
Sequence Output Tokens
Distribution(%)
[0, 512)
[512, 1024)
[1024,
8
)
[0, 512)
[512, 1024)
[1024,
8
)
[0, 512)
[512, 1024)
[1024,
8
)
[0, 128)
[128, 256)
[256,
8
)
Spider
-
-
-
-
-
-
-
-
-
-
-
-
GRAILQA
100.00
0.00
0.00
100.00
0.00
0.00
100.00
0.00
0.00
99.98
0.02
0.00
WebQsp
3.48
1.95
94.57
100.00
0.00
0.00
3.36
2.07
94.57
100.00
0.00
0.00
Russ
100.00
0.00
0.00
100.00
0.00
0.00
100.00
0.00
0.00
100.00
0.00
0.00
MTOP
0.00
100.00
0.00
100.00
0.00
0.00
0.00
100.00
0.00
100.00
0.00
0.00
WikiTableQuestions
48.00
31.15
20.86
100.00
0.00
0.00
47.08
31.70
21.22
99.98
0.02
0.00
WikiSQL
61.49
26.00
12.51
100.00
0.00
0.00
59.57
27.43
13.00
99.96
0.03
0.01
ComWebQ
0.85
16.02
83.13
100.00
0.00
0.00
0.85
13.07
86.08
99.43
0.57
0.00
HybridQA
-
-
-
-
-
-
-
-
-
-
-
-
FeTaQA
65.40
28.01
6.59
100.00
0.00
0.00
63.26
29.51
7.24
100.00
0.00
0.00
DART
100.00
0.00
0.00
100.00
0.00
0.00
100.00
0.00
0.00
100.00
0.00
0.00
ToTTo
-
-
-
-
-
-
-
-
-
-
-
-
MultiWoZ
100.00
0.00
0.00
98.71
1.29
0.00
24.82
74.93
0.24
0.00
100.00
0.00
KVRET
66.14
33.86
0.00
100.00
0.00
0.00
66.14
33.86
0.00
100.00
0.00
0.00
SParC
-
-
-
-
-
-
-
-
-
-
-
-
CoSQL
-
-
-
-
-
-
-
-
-
-
-
-
SQA
62.54
30.92
6.54
100.00
0.00
0.00
61.37
32.05
6.58
93.69
5.68
0.63
TabFact
64.59
28.01
7.40
100.00
0.00
0.00
62.55
29.35
8.10
100.00
0.00
0.00
FEVEROUS
-
-
-
-
-
-
-
-
-
-
-
-
SQL2Text
100.00
0.00
0.00
100.00
0.00
0.00
100.00
0.00
0.00
100.00
0.00
0.00
Logic2Text
100.00
0.00
0.00
100.00
0.00
0.00
100.00
0.00
0.00
100.00
0.00
0.00
Table 16: Input and output length for each task’s test set.
0
1000
2000
3000
4000
input tokens
0.0000
0.0002
0.0004
0.0006
0.0008
0.0010
0.0012
0.0014
0.0016
Density
Length distribution & model performance(T5 large)
36
38
40
42
44
46
48
50
Acc.
wikitq
Figure 6: Length effect on WikiTableQuestion.
style), make our input in that format and directly
test on T0 3B. The speciﬁc instructions are shown
below.
Spider
Given database schema "[linearized database
schema]". Can you tell me the SQL for "[request
]"?
WikiTQ
I know that the answer to "[request]" is in "[
linearized table]". Can you tell me what it is?
DART
Put the triples together to form a sentence: [
relation triples]
MultiWoZ
Known ontology "[ontology]", the dialogue state
when "[dialogue history and current request]" is
given
TabFact
Suppose "[linearized table]" Can we infer that
"[statement]"?
SQL2Text
Paraphrase "[SQL]" to natural language:
D.4
GPT3 and Codex Details
D.4.1
Hyperparameter Settings
Temperature
For GPT3 and Codex, we set the
decoding temperature to 0 (i.e., greedy decoding
without sampling) for Spider, WikiTQ, MultiWoZ
and TabFact. We observe a drop of 10% in the
exact match metric when set the temperature to 1
by default in OpenAI. For Codex, we tune the tem-
perature from 0 to 1 in a step of 0.1 for DART,
SQL2Text, and no signiﬁcant difference is ob-
served. For GPT3, we do not tune on that to stay
within our budget.
Max output length
We set max output length to
256 for Spider, WikiTQ, MultiWoZ and SQL2Text,
while 4 for TabFact to contain more length in the
input side(the concept of max length in GPT3 and
Codex is the sum of input tokens length and output
tokens length). We set “\n” as the stop token.
D.4.2
Prompts
We use simple prompt words for each task to con-
catenate the request, linearized structured knowl-
edge, and context together.
For example, for
each example in WikiTQ, we format it as “ex-
amples\n\n[linearized table] || Write a answer for
[request] \nThe answer is:”, and make GPT3 and
Codex make the completion as prediction. We
do experiments on Spider with different format of
forming structured knowledge (e.g., linearization,
description), but get a similar result. Better us-


age of GPT3 and Codex under the UNIFIEDSKG
framework is an interesting direction.
D.5
Human Evaluation
Participants of our human evaluation are eight of
the authors of this paper. They are familiar with
the tasks being evaluated. The human evaluation
guideline is shown below.
## General Guideline
1. Each line is a dev set sample, with some
inputs (detailed below), a human reference (
seq_out) shown in blue, and three model outputs
named model1, model2, and model3.
2. Each model output receives a 0-1 score (0
stands for incorrect, and 1 stands for correct).
By "correct" we mean "responding to the user
request properly and correctly, without grammar
or wording mistakes".
3. When an output is incorrect, you specify the
type(s) of error, e.g., 1) missing information,
2) contradiction, 3) hallucination, and 4)
ungrammatical.
## Task-Specific Details
### DART
1. Task: triples-to-text generation.
2. struct_in: a set of relation-triples joined
by ``|``. Each relation-triple is of form ``
entityA : relation : entityB``.
### FeTaQA
1. Task: free-form QA
2. question: a question about the table.
3. table: a table represented as a dictionary:
{"header": [header item, ...], "rows": [[cell
value, ...], ...]}.
4. meta: table_page_title | table_section_title
### KVRET
1. Task: dialogue system
2. dialogue: a dialogue represented as a
dictionary: {"driver": [request1, ...], "
assistant": [response1, ...]}, the last response
of the assistant is the human reference.
3. kb: a knowledge base represented as a
dictionary: {"header": [header item, ...], "rows
": [[cell value, ...], ...]}.
### Logic2Text
1. Task: logic expression to text translation
2. table: a table represented as a dictionary:
{"caption": table caption, "header": [header
item, ...], "rows": [[cell value, ...], ...]}.
3. logic_str: logic expression of a statement.
### SQL2Text
1. Task: SQL to text translation
2. query: SQL.
### ToTTo
1. Task: highlighted-table-to-text generation.
2. table_page_title and section: table meta
information.
3. Visualization of highlighted tables is
provided in ``totto_vis/``.
D.6
Hyperparameters
Shown in Table 17. For semantic parsing tasks, the
decoding was done under the greedy search, where
we set the beam size to 1 specially. For tasks with
a long linearized sequence, we used 1024 as input
length to hold the maximum of input; reasons are
explained in App. C.
E
Training Details
Here we show comparisons of ﬁnetuning and
preﬁx-tuning on aspect of training. For preﬁx-
tuning, we use random initialization as done by Li
and Liang (2021). In general, preﬁx-tuning needs
more steps than ﬁnetuning but has the ability to
reach comparable results with continued training.
F
Task Uniﬁcation
F.1
Term Deﬁnition
Highlighted tables
A highlighted table contains
a table, table metadata (such as the title), and a set
of highlighted cells which entails the text descrip-
tion (Parikh et al., 2020).
Relation-triples
Relation triples are a set of
subject-predicate-object triples to capture rich re-
lationships in the data. Many data-to-text tasks
such as DART (Nan et al., 2021b) take these rela-
tion triples as inputs and generate natural language
from them.
Knowledge Graph
A knowledge graph is a
multi-relational graph composed of entities (nodes)
and relations (different types of edges). Each edge
is represented as a triple of the form (head entity,
relation, tail entity), also called a fact, indicating
that two entities are connected by a speciﬁc relation
(Wang et al., 2017).
Dialogue State and Ontology
A dialogue state
st at any turn t in a dialogue comprises the sum-
mary of the dialogue history until turn t, such that
st contains all sufﬁcient information for the system
to choose the next action. (Williams et al., 2016)
Speciﬁcally, it captures the user goals in the con-
versation in the form of (slot, value) pairs. The
set of possible slots is predeﬁned in the ontology
O, typically domain-dependent, while the values
assumed by each slots are provided by the user as
a dialogue goal.


Task type
Task
Input length
Batch size
Beam size
Semantic Parsing
Spider (Yu et al., 2018)
512
32
1
GrailQA (Gu et al., 2021)
512
32
4
WebQSP (Yih et al., 2016)
1024
32
4
MTOP (Li et al., 2021)
1024
32
4
Question Answering
WikiSQL (Zhong et al., 2017)
1024
128
4
WikiTQ (Pasupat and Liang, 2015)
1024
128
4
CompWebQ (Talmor and Berant, 2018)
1024
32
4
HybridQA (Chen et al., 2020c)
1024
32
4
MultiModalQA (Talmor et al., 2021)
1024
32
4
FeTaQA (Nan et al., 2021a)
512
32
4
Data-to-Text
DART (Nan et al., 2021b)
512
32
4
ToTTo (Parikh et al., 2020)
512
32
4
MultiWoZ2.1 (Eric et al., 2019)
1024
32
4
KVRET (Eric et al., 2017)
1024
32
4
Conversational
SParC (Yu et al., 2019b)
512
32
1
CoSQL (Yu et al., 2019a)
512
32
1
SQA (Iyyer et al., 2017)
1024
128
4
Fact Veriﬁcation
TabFact (Chen et al., 2020b)
1024
128
4
FEVEROUS (Aly et al., 2021)
1024
32
4
High-ﬁdelity NLG
SQL2Text (Shu et al., 2021)
512
32
4
Logic2Text (Chen et al., 2020d)
512
32
4
Table 17: Hyperparameters for each SKG task.
F.2
Linearization
• Tables. Following Liu et al. (2021), we linearize
the table into a sequence. By inserting several
special tokens to indicate the table boundaries, a
linearized table can be represented as “col: c1, ...,
cN row 1 : r1 row 2 : r2... rM ”, N and M are
the number of columns and rows.
• Highlighted tables.
Following Parikh et al.
(2020), we represent each highlighted cell by
concatenating its value, column headers, and row
headers. The table is represented as the concate-
nation of the page title, section title, and repre-
sentations of all highlighted cells.
• Relation-triples and knowledge graphs. Fol-
lowing Nan et al. (2021b), each relation-triple
is linearized as “sub : rela : obj”, and different
triples are joined by “ | ”. The subgraph retrieved
from the knowledge graph is treated as a list of
relation-triples and we use the same formulation.
• Ontology. Following Hosseini-Asl et al. (2020)
and Lin et al. (2021), for each slot in ontology,
each slot along with its all possible values is for-
matted as “slot : value1, ... valueslotn ”, different
slot-values are joined by “ | ”
F.3
Output Format
When the output is natural language or formal lan-
guage we do not modify it because it is already in
sequence format; a set of answers, we use a comma
followed by a space to join the answers; a Boolean
value, we map True to “entailed” and False to “re-
futed”; a dialogue state, we follow Hosseini-Asl
et al. (2020) to place its slot-value pairs sequen-
tially.
G
Input and Output Examples for Each
Task
G.1
Spider
Structured Input:
| concert_singer | stadium : stadium_id ,
location , name , capacity , highest , lowest ,
average | singer : singer_id , name , country ,
song_name , song_release_year , age , is_male |
concert : concert_id , concert_name , theme ,
stadium_id , year | singer_in_concert :
concert_id , singer_id
Request Input:
How many singers do we have?
Sequence Output:
select count(*) from singer
G.2
GRAILQA
Structured Input:
soviet red army: m.06dr9 | organization.
organization.founders government.
governmental_body.jurisdiction organization.
organization_founder.organizations_founded


Task
Finetune
Preﬁx-tuning
Spider
16500
100000
GrailQA
17000
78000
WebQSP
1500
8000
MTOP
30000
60000
WikiSQL
8500
80000
WikiTQ
1500
16000
CompWebQ
3500
27000
HybridQA
7000
30000
MultiModalQA
6000
40000
FeTaQA
11000
20000
DART
7000
250000
ToTTo
12000
>250000
MultiWoZ2.1
6000
40000
KVRET
4000
40000
SParC
2000
6400
CoSQL
38000
100000
SQA
27000
>250000
TabFact
8000
210000
FEVEROUS
1200
40000
SQL2Text
3000
10000
Logic2Text
3500
10000
Table 18:
The comparison of approximate training
steps ﬁnetuning and preﬁx-tuning used to reach the de-
cent performance on T5 base. >250000 means we stop
the training due to time limitation. Preﬁx-tuning needs
more steps to converge and converges to comparable
performances.
military.military_service.military_person
government.political_party_tenure government.
national_anthem_of_a_country visual_art.
art_subject.artwork_on_the_subject government.
government_agency government.
governmental_jurisdiction.government people.
deceased_person.place_of_burial people.
deceased_person.date_of_death people.person.
children people.person.parents people.person.
height_meters government.
government_position_held.office_holder
government.government people.person people.
person.sibling_s people.person.quotations people.
person.gender
Request Input:
the person who founded the soviet red army also
founded what government agency?
Sequence Output:
(AND government.government_agency (JOIN
organization.organization.founders (JOIN (R
organization.organization.founders) m.06dr9)))
G.3
CompWebQ
Structured Input:
Liam Hemsworth celebrities.celebrity.
sexual_relationships..celebrities.
romantic_relationship.celebrity Liam Hemsworth |
Liam Hemsworth type.object.type tv.tv_actor |
Liam Hemsworth film.actor.film..film.performance.
character Billy The Kid | Liam Hemsworth film.
actor.film..film.performance.film The Hunger
Games |
(omitted to save space)
| Liam Hemsworth type.object.type celebrities.
celebrity | Cut Bank film.film.cinematography
Ben Richardson | Cut Bank film.film.language
English Language | Cut Bank film.film.edited_by
Carol Littleton | Liam Hemsworth film.actor.film
..film.performance.film The Hunger Games:
Mockingjay, Part 1 | Liam Hemsworth film.actor.
film..film.performance.film Timeless
Request Input:
What movie was produced by Dan Cohen and
features Liam Hemsworth as an actor?
Sequence Output:
Cut Bank
G.4
WebQsp
Structured Input:
Spain: m.06mkj | m.06mkj location.location.
contains m.0g3qgy | m.06mkj location.location.
contains m.02qf5mh | m.0j5_3sv government.
government_position_held.
office_position_or_title m.0j5_3sz | m.06mkj
location.location.contains m.02zb43k |
(omitted to save space)
| m.06mkj government.governmental_jurisdiction.
governing_officials m.010wswjc | m.06mkj
location.location.contains m.09k5hy | m.010wswjc
government.government_position_held.
office_position_or_title m.0j5_3sz | m.06mkj
location.location.contains m.02z98t5 | m.06mkj
location.location.contains m.03qcr60
Request Input:
what is the king of spain's name?
Sequence Output:
(JOIN (R government.government_position_held.
office_holder) (AND (JOIN government.
government_position_held.time_macro 2015^^http
://www.w3.org/2001/XMLSchema#date) (AND (JOIN
government.government_position_held.
office_position_or_title m.0j5_3sz) (JOIN (R
government.governmental_jurisdiction.
governing_officials) m.06mkj))))
G.5
MTOP
Structured Input:
IN:GET: MESSAGE, WEATHER, ALARM, INFO_RECIPES,
STORIES_NEWS, REMINDER, RECIPES, EVENT,
CALL_TIME, LIFE_EVENT, INFO_CONTACT, CONTACT,
TIMER, REMINDER_DATE_TIME, AGE, SUNRISE,
EMPLOYER, EDUCATION_TIME, JOB, AVAILABILITY,
(omitted to save space)
IN:PREVIOUS: TRACK_MUSIC | IN:HOLD: CALL | IN:
SKIP: TRACK_MUSIC | IN:LIKE: MUSIC | IN:RESTART:
TIMER | IN:RESUME: TIMER, CALL, MUSIC | IN:
MERGE: CALL | IN:REPLAY: MUSIC | IN:LOOP: MUSIC
| IN:STOP: MUSIC, SHUFFLE_MUSIC | IN:UNLOOP:
MUSIC | IN:CANCEL: MESSAGE, CALL | IN:REWIND:
MUSIC | IN:REPEAT: ALL_MUSIC, ALL_OFF_MUSIC |
IN:FAST: FORWARD_MUSIC | IN:DISLIKE: MUSIC | IN:
DISPREFER: | IN:HELP: REMINDER | IN:FOLLOW:
MUSIC


Request Input:
call Nicholas and Natasha
Sequence Output:
[IN:CREATE_CALL [SL:CONTACT Nicholas ] [SL:
CONTACT Natasha ] ]
G.6
WikiSQL
Structured Input:
col : player | no. | nationality | position |
years in toronto | school/club team row 1 :
antonio lang | 21 | united states | guard-
forward | 1999-2000 | duke row 2 : voshon lenard
| 2 | united states | guard | 2002-03 |
minnesota row 3 : martin lewis | 32, 44 | united
states | guard-forward | 1996-97 | butler cc (
ks) row 4 : brad lohaus | 33 | united states |
forward-center | 1996 | iowa row 5 : art long |
42 | united states | forward-center | 2002-03 |
cincinnati row 6 : john long | 25 | united
states | guard | 1996-97 | detroit row 7 : kyle
lowry | 3 | united states | guard | 2012-present
| villanova
Request Input:
what position does the player who played for
butler cc (ks) play?
Sequence Output:
guard-forward
G.7
WikiTableQuestions
Structured Input:
col : team | county | wins | years won row 1 :
greystones | wicklow | 1 | 2011 row 2 :
ballymore eustace | kildare | 1 | 2010 row 3 :
maynooth | kildare | 1 | 2009 row 4 : ballyroan
abbey | laois | 1 | 2008 row 5 : fingal ravens |
dublin | 1 | 2007 row 6 : confey | kildare | 1
| 2006 row 7 : crettyard | laois | 1 | 2005 row
8 : wolfe tones | meath | 1 | 2004 row 9 :
dundalk gaels | louth | 1 | 2003
Request Input:
which team won previous to crettyard?
Sequence Output:
wolfe tones
G.8
HybridQA
Structured Input:
col : position | athlete | nationality | time
row 1 : 1 | patrick makau musyoki | kenya |
2:03.38 row 2 : 2 | stephen kwelio chemlany |
kenya | 2:07.55 row 3 : 3 | edwin kimaiyo |
kenya | 2:09.50 row 4 : 4 | felix limo | kenya |
2:10.38 row 5 : 5 | scott overall | united
kingdom | 2:10.55 row 6 : 6 | ricardo serrano |
spain | 2:13.32 row 7 : 7 | pedro nimo | spain |
2:13.34 row 8 : 8 | simon munyutu | france |
2:14.20 row 9 : 9 | driss el himer | france |
2:14.46 row 10 : 10 | hendrick ramaala | south
africa | 2:16.00passages: ricardo serrano (
athlete): at the 2011 iaaf world cross country
championships he was 89th overall . his marathon
debut followed later that year and he was sixth
at the 2011 berlin marathon with a time of
2:13.32 hours . | spain: with an area of 505,990
km2 ( 195,360 sq mi ) , spain is the largest
country in southern europe , the second largest
country in western europe and the european union
, and the fourth largest country in the
european continent . by population ( about 47
million ) , spain is the sixth largest in europe
and the fifth in the european union . |
Request Input:
what place was achieved by the person who
finished the berlin marathon in 2:13.32 in 2011
the first time he competed in a marathon ?
Sequence Output:
sixth
G.9
MultiModalQA
Structured Input:
ben piazza | filmography col : year | title |
role | notes row 1 : 1957 | a dangerous age |
david | row 2 : 1959 | the hanging tree | rune |
row 3 : 1962 | no exit | camarero | row 4 :
1970 | tell me that you love me, junie moon |
jesse | row 5 : 1972 | the outside man | desk
clerk | row 6 : 1973 | the candy snatchers |
avery | row 7 : 1976 | the bad news bears | bob
whitewood | row 8 : 1977 | i never promised you
a rose garden | jay blake | row 9 : 1979 |
nightwing | roger piggott | row 10 : 1979 | the
concorde ... airport '79 | associate | tv
version, uncredited row 11 : 1980 | the blues
brothers | father | row 12 : 1982 | waltz across
texas | bill wrather | row 13 : 1985 | mask |
mr. simms | row 14 : 1988 | clean and sober |
kramer | row 15 : 1990 | rocky v | doctor |
uncredited row 16 : 1991 | guilty by suspicion |
darryl zanuck |
Request Input:
for which film did ben piazza play the role of
mr. simms?
Sequence Output:
mask
G.10
FeTaQA
Structured Input:
andy karl | awards and nominations col : year |
award | category | work | result row 1 : 2013 |
drama desk award | outstanding featured actor in
a musical | the mystery of edwin drood |
nominated row 2 : 2013 | broadway.com audience
awards | favorite onstage pair (with jessie
mueller) | the mystery of edwin drood |
nominated row 3 : 2014 |


(omitted to save space)
| groundhog day | nominated row 15 : 2017 | tony
award | best actor in a musical | groundhog day
| nominated row 16 : 2017 | drama desk award |
outstanding actor in a musical | groundhog day |
won row 17 : 2017 | drama league award |
distinguished performance | groundhog day |
nominated row 18 : 2017 | outer critics circle
award | outstanding actor in a musical |
groundhog day | won
Request Input:
when did andy karl win the olivier award and for
which of his work?
Sequence Output:
for his performance in groundhog day, andy karl
received the 2017 olivier award for best actor
in a musical.
G.11
DART
Structured Input:
Mars Hill College : joined : 1973 | Mars Hill
College : location : Mars Hill, North Carolina
Request Input:
None
Sequence Output:
A school from Mars Hill, North Carolina, joined
in 1973.
G.12
ToTTo
Structured Input:
<page_title> List of Governors of South Carolina
</page_title> <section_title> Governors under
the Constitution of 1868 </section_title> <table
> <cell> 76 <col_header> # </col_header> <
col_header> 74 </col_header> <col_header> 75 </
col_header> </cell> <cell> Daniel Henry
Chamberlain <col_header> Governor </col_header>
<row_header> 76 </row_header> </cell> <cell>
December 1, 1874 <col_header> Took Office </
col_header> <row_header> 76 </row_header> </cell
> </table>
Request Input:
None
Sequence Output:
Daniel Henry Chamberlain was the 76th Governor
of South Carolina from 1874.
G.13
MultiWoZ2.1
Structured Input:
hotel-pricerange: cheap, dontcare, expensive,
moderate; hotel-type: guesthouse, hotel; hotel-
parking: dontcare, free, no, yes; hotel-book day:
friday, monday, saturday, sunday, thursday,
tuesday, wednesday; hotel-book people: 1, 2, 3,
4, 5, 6, 7, 8; hotel-book stay: 1, 2, 3, 4, 5, 6,
7, 8; hotel-area: centre, dontcare, east, north,
south, west; hotel-stars: 0, 1, 2, 3, 4, 5,
dontcare; hotel-internet: dontcare, no, yes;
hotel-name: none; train-destination: none; train-
day: dontcare, friday, monday, saturday, sunday,
thursday, tuesday, wednesday; train-departure:
none; train-arriveby: none; train-book people: 0,
1, 10, 15, 2, 3, 4, 5, 6, 7, 8, 9; taxi-
destination: none; taxi-departure: none; taxi-
leaveat: none; train-leaveat: none; attraction-
area: cambridge, centre, dontcare, east, north,
south, west; restaurant-pricerange: cheap,
dontcare, expensive, moderate; restaurant-area:
centre, east, north, south, west; restaurant-
food: none; attraction-name: none; restaurant-
name: none; attraction-type: architecture, boat,
church, cinema, college, concerthall,
entertainment, hotspot, multiple sports, museum,
nightclub, park, special, swimmingpool, theatre
; restaurant-book day: friday, monday, saturday,
sunday, thursday, tuesday, wednesday;
restaurant-book people: 1, 2, 3, 4, 5, 6, 7, 8;
restaurant-book time: none; taxi-arriveby: none;
Request Input:
i am looking to book a train that is leaving
from cambridge to bishops stortford on friday .
Context:
booking was successful . reference number is :
bmukptg6 . can i help you with anything else
today ? | friday and can you book it for me and
get a reference number ? | what day will you be
staying ? | could you book the wartworth for 1
night , 1 person ? | if you would like something
cheap , i recommend the allenbell . for
something moderate -ly priced , i would
recommend the warkworth house . | that does not
matter as long as it has free wifi and parking .
| i can help you with that . what is your price
range ? | i need to book a hotel in the east
that has 4 stars . | none
Sequence Output:
hotel pricerange none, hotel type none, hotel
parking yes, hotel book day friday, hotel book
people 1, hotel book stay 1, hotel area east,
hotel stars 4, hotel internet yes, hotel name
wartworth, train destination bishops stortford,
train day friday, train departure cambridge,
train arriveby none, train book people none,
taxi destination none, taxi departure none, taxi
leaveat none, train leaveat none, attraction
area none, restaurant pricerange none,
restaurant area none, restaurant food none,
attraction name none, restaurant name none,
attraction type none, restaurant book day none,
restaurant book people none, restaurant book
time none, taxi arriveby none
G.14
KVRET
Structured Input:


col : poi | poi_type | address | distance |
traffic_info row 1 : sigona farmers market |
grocery store | 638 amherst st | 3 miles | car
collision nearby row 2 : cafe venetia | coffee
or tea place | 269 alger dr | 1 miles | car
collision nearby row 3 : 5672 barringer street |
certain address | 5672 barringer street | 5
miles | no traffic row 4 : valero | gas station
| 200 alester ave | 2 miles | road block nearby
row 5 : stanford childrens health | hospital |
899 ames ct | 5 miles | moderate traffic row 6 :
palo alto garage r | parking garage | 481
amaranta ave | 1 miles | moderate traffic row 7 :
teavana | coffee or tea place | 145 amherst st
| 1 miles | road block nearby row 8 : willows
market | grocery store | 409 bollard st | 5
miles | no traffic
Request Input:
ok, please give me directions via a route that
avoids all heavy_traffic.
Context:
valero is located at 200_alester_ave. | address
to the gas_station
Sequence Output:
since there is a road_block_nearby, i found
another route for you and i sent it on your
screen
G.15
SParC
Structured Input:
| flight_2 | airlines : uid , airline ,
abbreviation , country | airports : city ,
airportcode , airportname , country ,
countryabbrev | flights : airline , flightno ,
sourceairport , destairport
Request Input:
What is the country corresponding it?
Context:
Of these, which is Jetblue Airways? | What are
all the airlines?
Sequence Output:
select country from airlines where airline = "
JetBlue Airways"
G.16
CoSQL
Structured Input:
| car_1 | continents : contid , continent |
countries : countryid , countryname , continent
| car_makers : id , maker , fullname , country |
model_list : modelid , maker , model |
car_names : makeid , model , make | cars_data :
id , mpg , cylinders , edispl , horsepower ,
weight , accelerate , year
Request Input:
Which models are the most popular?
Context:
How many car models are produced by each maker?
List the count and the maker full name.
Sequence Output:
select t1.model from model_list as t1 join
car_makers as t2 on t1.maker = t2.id group by t2.
id order by count ( * ) desc limit 1
G.17
SQA
Structured Input:
col : date | number | type | captain |
casualties | sunk by row 1 : 4 may 1943 | u-209
| viic | heinrich brodda | 46 | catalina flying-
boat of no. 5 squadron row 2 : 5 may 1943 | u
-638 | viic | oskar staudinger | 44 | hms
sunflower row 3 : 5 may 1943 | u-531 | ixc/40 |
herbert neckel | 54 | hms vidette row 4 : 6 may
1943 | u-192 | ixc/40 | werner happe | 55 | hms
loosestrife row 5 : 6 may 1943 | u-125 | ixc |
ulrich folkers | 54 | "hms oribi | hms snowflake
" row 6 : 6 may 1943 | u-630 | viic | werner
winkler | 47 | hms vidette row 7 : 6 may 1943 |
u-438 | viic | heinrich hensohn | 48 | hms
pelican
Request Input:
which captain was not oskar staudinger?
Context:
who were the captains of those boats? | what
boats were lost on may 5?
Sequence Output:
herbert neckel
G.18
TabFact
Structured Input:
col : round | clubs remaining | clubs involved |
winners from previous round | new entries this
round | leagues entering at this round row 1 :
first round | 156 | 86 | none | 86 | tff third
league & turkish regional amateur league row 2 :
second round | 113 | 108 | 43 | 65 | s\u00fcper
lig & tff first league & tff second league row
3 : third round | 59 | 54 | 54 | none | none row
4 : fourth round | 32 | 32 | 27 | 5 | s\
u00fcper lig row 5 : fifth round | 16 | 16 | 16
| none | none row 6 : group stage | 8 | 8 | 8 |
none | none row 7 : semi - finals | 4 | 4 | 4 |
none | none row 8 : final | 2 | 2 | 2 | none |
none
Request Input:
during the third round of the turkish cup ,
there be no new entry during that stage
Sequence Output:
entailed


G.19
FEVEROUS
Structured Input:
col : no. | title | narrator | aired between |
original air date | us viewers row 1 : 1 | "
magic is coming" | giancarlo esposito | "a land
without magic" "broken" | september 30, 2012
(2012-09-30) | 6.04 row 2 : 2 | "the price of
magic" | alan dale | "selfless, brave and true"
"lacey" | april 14, 2013 (2013-04-14) | 5.17 row
3 : 3 | "journey to neverland" | alfred molina
| "and straight on 'til morning" "the heart of
the |
(omitted to save space)
| "dark swan rises: a once upon a time fan |
howard parker | "operation mongoose, part 2" "
the | september 27, 2015 (2015-09-27) | 3.20 row
8 : 8 | "evil reigns once more" | howard parker
| "an untold story" "the savior" | september 25,
2016 (2016-09-25) | 2.86 row 9 : 9 | "the final
battle begins" | howard parker | "the song in
your heart" "the final battle" | may 14, 2017
(2017-05-14) | 2.82
Request Input:
list of once upon a time (debuted october 23,
2011) episodes consists of 10 specials, one of
the narrators alan dale.
Sequence Output:
refuted
G.20
SQL2Text
Structured Input:
None
Request Input:
SELECT Fname FROM FACULTY WHERE Rank = "
Professor" ORDER BY Fname
Sequence Output:
What are the first names for all faculty
professors, ordered by first name?
G.21
Logic2Text
Structured Input:
Caption: list of tallest structures in the world
- 300 to 400 metres | planned pinnacle height,
expected year of completion, structural type,
country, town
Request Input:
eq { count { filter_eq { all_rows ; expected
year of completion ; 2006 } } ; 3 } = true
Sequence Output:
three of the tallest structures will be
completed in 2006 .
H
Case study
H.1
Spider case
H.1.1
Case 1:
Structured Input:
| concert_singer | stadium : stadium_id ,
location , name , capacity , highest , lowest ,
average | singer : singer_id , name , country ,
song_name , song_release_year , age , is_male |
concert : concert_id , concert_name , theme ,
stadium_id , year | singer_in_concert :
concert_id , singer_id
Request Input:
What are the names of all stadiums that did not
have a concert in 2014?
Sequence Output:
select name from stadium except select t2.name
from concert as t1 join stadium as t2 on t1.
stadium_id = t2.stadium_id where t1.year = 2014
T5-base prediction (incorrect):
select name from stadium except select
stadium_name from concert where year = 2014
T5-large prediction (correct):
select name from stadium except select t2.name
from concert as t1 join stadium as t2 on t1.
stadium_id = t2.stadium_id where t1.year = 2014
T5-3B prediction (correct):
select name from stadium except select t2.name
from concert as t1 join stadium as t2 on t1.
stadium_id = t2.stadium_id where t1.year = 2014
H.1.2
Case 2:
Structured Input:
| concert_singer | stadium : stadium_id ,
location , name , capacity , highest , lowest ,
average | singer : singer_id , name , country ,
song_name , song_release_year , age , is_male |
concert : concert_id , concert_name , theme ,
stadium_id , year | singer_in_concert :
concert_id , singer_id
Request Input:
What is the name and capacity for the stadium
with highest average attendance?
Sequence Output:
select name, capacity from stadium order by
average desc limit 1
T5-base prediction (incorrect):
select name, capacity from stadium order by avg(
amount) desc limit 1
T5-large prediction (correct):
select name, capacity from stadium order by
average desc limit 1


T5-3B prediction (correct):
select name, capacity from stadium order by
average desc limit 1
H.1.3
Case 3:
Structured Input:
| pets_1 | student : stuid , lname , fname , age
, sex , major , advisor , city_code | has_pet :
stuid , petid | pets : petid , pettype ( cat ,
dog ) , pet_age , weight
Request Input:
Find the first name of students who have cat or
dog pet.
Sequence Output:
select distinct t1.fname from student as t1 join
has_pet as t2 on t1.stuid = t2.stuid join pets
as t3 on t3.petid = t2.petid where t3.pettype =
'cat' or t3.pettype = 'dog'
T5-base prediction (incorrect):
select t1.fname from student as t1 join has_pet
as t2 on t1.stuid = t2.stuid where t2.pettype =
"cat" or t2.pettype = "dog"
T5-large prediction (incorrect):
select t1.fname from student as t1 join has_pet
as t2 on t1.stuid = t2.stuid where t2.pettype =
"cat" or t2.pettype = "dog"
T5-3B prediction (correct):
select t1.fname from student as t1 join has_pet
as t2 on t1.stuid = t2.stuid join pets as t3 on
t2.petid = t3.petid where t3.pettype = "cat" or
t3.pettype = "dog"
H.2
FeTaQA case
Structured Input:
te aroha (new zealand electorate) | 1890
election col : party | party | candidate | votes
| % | [non utf-8 token] row 1 : - | independent
| william shepherd allen | 786 | 56.34 | - row
2 : - | liberal | william fraser | 609 | 43.65 |
- row 3 : majority | majority | majority | 175
| 12.54 | - row 4 : turnout | turnout | turnout
| 1,395 | 48.60 | - row 5 : registered electors
| registered electors | registered electors |
2,870 | - | -
Request Input:
what were the voting results in the te aroha
election by the candidate?
Sequence Output:
votes for allen and fraser were 786 and 609
respectively.
T5-base prediction (incorrect):
the te aroha election was won by william
shepherd allen with a majority of 175.
T5-large prediction (incorrect):
in the te aroha election, allen won with a
majority of 175 votes.
T5-3B prediction (correct):
in the 1890 te aroha election, the result was a
victory for the independent candidate, william
shepherd allen, who received 786 votes against
609 for the liberal candidate, william fraser.
H.3
KVRET case
H.3.1
Case 1:
Structured Input:
col : location | monday | tuesday | wednesday |
thursday | friday | saturday | sunday | today
row 1 : danville | clear skies, low of 30f, high
of 50f | cloudy, low of 20f, high of 40f |
raining, low of 80f, high of 100f | dew, low of
80f, high of 90f | clear skies, low of 70f, high
of 80f | frost, low of 40f, high of 60f |
overcast, low of 20f, high of 30f | monday row 2
:... row 6 : alhambra | windy, low of 90f, high
of 100f | hail, low of 90f, high of 100f |
clear skies, low of 80f, high of 90f | rain, low
of 70f, high of 80f | stormy, low of 70f, high
of 80f | rain, low of 60f, high of 70f | hail,
low of 40f, high of 60f | monday ...
Request Input:
alhambra please.
Context Input:
what city are you inquiring about? | is it going
to be raining this weekend
Sequence Output:
it will be raining on saturday and hailing on
sunday in alhambra.
T5-base prediction (correct):
it will be raining on saturday and hailing on
sunday in alhambra.
T5-large prediction (incorrect):
it will not rain in alhambra this weekend.
T5-3B prediction (incorrect):
there will be no rain this weekend in alhambra.
H.3.2
Case 2:
Structured Input:
col : event | time | date | room | agenda |
party row 1 : tennis activity | 5pm | wednesday
| - | - | mother row 2 : taking medicine | 9am |
the 3rd | - | - | sister row 3 : swimming
activity | 11am | friday | - | - | father row 4 :
conference | 6pm | the 8th | 100 conference
room | go over budget | infrastructure team row
5 : meeting | 4pm | the 10th | conference room
102 | go over quarterly report | infrastructure
team row 6 : dentist appointment | 10am | the 13
th | - | - | jeff row 7 : dentist appointment |
11am | the 5th | - | - | jeff row 8 : lab
appointment | 2pm | the 4th | - | - | sister


Request Input:
when is the date and time of my
dentist_appointment?
Context Input:
None
Sequence Output:
your dentist_appointment is at 10am on the_13th.
T5-base prediction (correct):
you have two dentist_appointments scheduled. one
on the_5th at 11am and another on the_13th at
10am.
T5-large prediction (correct):
your dentist_appointment is on the_13th at 10 am.
T5-3B prediction (incorrect):
you have three dentist_appointments scheduled.
one on the_5th at 11am, another on the_13th at
10am, and the last is on the_5th at 11am.
H.4
SQL2Text case
Query:
SELECT company , main_industry FROM company
WHERE headquarters != 'USA'
Sequence Out:
Show the company name and the main industry for
all companies whose headquarters are not from
USA.
T5-base prediction (incorrect):
What are the companies and main industries of
companies whose headquarters are not "USA"?
T5-large prediction (correct):
What are the companies and main industries of
the companies whose headquarters are not located
in the United States?
T5-3B prediction (incorrect):
What are the main industries of companies whose
headquarters are not in the USA?
H.5
DART case
Structured Input:
Cotto : eattype : coffee shop | Cotto : food :
English | Cotto : pricerange : more than [non
utf-8 token]30 | Cotto : customer rating : high
| Cotto : area : city centre | Cotto : near :
The Portland Arms
Sequence Output:
The Cotto is a highly rated English coffee shop
in the city centre near The Portland Arms, and
prices range more than [non utf-8 token]30.
T5-base prediction (correct):
Cotto is a coffee shop providing English food in
the more than [non utf-8 token]30 price range.
It is located in the city centre. It is near The
Portland Arms. Its customer rating is high.
T5-large prediction (correct):
Cotto is a coffee shop providing English food in
the more than [non utf-8 token]30 price range.
It is located in the city centre. It is near The
Portland Arms. Its customer rating is high.
T5-3B prediction (correct):
Cotto is a coffee shop located in the city
centre near The Portland Arms. It serves English
food with a price range of more than [non utf-8
token]30 and has a high customer rating.
H.6
Logic2Text case
Structured Input:
Caption: 2008 san diego chargers season | week,
date, time, opponent, result, game site, nfl
recap, record
Request Input:
eq { count { filter_eq { filter_eq { all_rows ;
game site ; qualcomm stadium } ; time ; 5:15 pm }
} ; 3 } = true
Sequence Output:
in the 2008 san diego chargers season , among
the games that were played in qualcomm stadium ,
3 of them started at 5:15 pm .
T5-base prediction (incorrect):
in the 2008 san diego chargers season, when the
game was at qualcomm stadium, there were three
times the time was 5:15 pm.
T5-large prediction (incorrect):
in the 2008 san diego chargers season, when the
game was at qualcomm stadium, there were 3 times
the time was 5:15 pm.
T5-3B prediction (correct):
in the 2008 san diego chargers season, among the
games played at qualcomm stadium, 3 of them
started at 5:15 pm.
H.7
ToTTo case
Structured Input: See Figure 7.
Sequence Output:
Alisson Perticheto placed 18th at the 2013
Junior Worlds, 17th at the 2014 Four Continents
and 16th at the 2015 Four Continents.
T5-base prediction (incorrect):
Alisson Perticheto finished 18th at the Junior
Worlds and 17th at the Four Continents.


Figure 7: Visualized highted table for ToTTo case 1.
T5-large prediction (incorrect):
Alisson Perticheto placed 17th at the 2014 Four
Continents and 16th at the 2015 Junior Worlds.
T5-3B prediction (correct):
Alisson Perticheto finished 17th at the 2014
Four Continents, 16th at the 2015 Four
Continents, and 18th at the 2013 Junior Worlds.
I
Natural Language Template Examples
I.1
Spider Template
Overall Description Template:
{db id} contains tables such as {table1 name}, {
table2 name}
Primary Key Template:
{primary key} is the primary key.
Table Description Template:
Table {table name} has column such as {column 1
name}, {column 2 name}, ...
Foreign Keys Description Template:
The {column1 name} of {table 1} is the foreign
key of {column2 name} of {table 2}
I.2
TabFact Template
Template Examples:
Table 1-24143253-5:
{name} lost his spouse {deceased spouse} to {
cause of death} on {date of spouses death} after
{length of marriage} of marriage; they had {
children together} together; he is currently {
current marital status}
Table 2-14978398-2:
The {version} of song Comme j'ai mal has a
length of {length} in album {album} remixed by {
remixed by} in year {year}
Table 1-15187735-12:
On {date} in 1936 VFL Season, the home team {
home team} and away team {away team} had a game
at venue {venue} with a crowd of {crowd}; the
home team score is {home team score} and the
away team score is {away team score}
I.3
WikiSQL Template
Template Example:
Table 1-14240688-1:
in {year} were in division {division}, {league}
ranked {regular season}, made it to {playoffs}
of the playoffs, made it to <{open cup}> in the
open cup, and kept an average attendance of {avg
attendance}
Table 2-12997882-1:
On {date} in 2008 European Figure Skating, the
home team {home team} and away team {away team}
had a game at venue {venue} with a crowd of {
crowd}; the home team score is {home team score}
and the away team score is {away team score}
Table 1-13740746-1:
Episode number {ep no} of gerry anderson 's new
captain scarlet with a title of {title} is
directed by {director} and written by {written
by}; its original air date is {original air date
}; the production number is {production no}



INFO:root:--------------------
INFO:root:agent_dv_pdfreader is thinking...
INFO:root:Done
INFO:root:[blue]agent_dv_pdfreader: 
INFO:root:The first page of the PDF titled "UNIFIEDSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models" introduces the concept of Structured Knowledge Grounding (SKG). It highlights the challenges posed by the heterogeneous nature of SKG tasks, which include semantic parsing and question answering over various structured data sources. The authors propose a framework called UNIFIEDSKG that unifies 21 different SKG tasks into a text-to-text format. This approach aims to facilitate systematic research and benchmarking across these tasks, allowing for the application of models like T5 to achieve state-of-the-art performance. The paper also discusses the benefits of multi-task prefix-tuning and the challenges of zero-shot and few-shot learning in SKG contexts. The framework is open-sourced to encourage further research and development in the field.
INFO:root:Done
INFO:root:[green]User: 
INFO:root:
[red]Bye!
